{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>Hi! I'm Ricky Ding.  This is a site where I keep my study notes. (Physics, Neuroscience, AI...)</p> <ul> <li>LinkedIn, GitHub, Email, WeChat (166 1977 1943)</li> </ul>"},{"location":"DeepSeek.html","title":"DeepSeek","text":""},{"location":"DeepSeek.html#1-token-word-embedding","title":"1. Token (word) Embedding","text":"<ul> <li> <p>torch.nn.functional.embedding</p> </li> <li> <p>In LLMs:</p> <ul> <li>Each token (word) is mapped to an integer</li> <li>This integer is then mapped to a vector</li> </ul> </li> <li>The code below is a module that maps integers (0 ~ 102400) to vectors (dim = 2048)</li> <li> <p>In this parallel version:</p> <ul> <li><code>world_size = number of GPUs</code></li> <li><code>rank = GPU index</code></li> </ul> </li> <li> <p>Given:  </p> <ul> <li>$ W \\in \\mathbb{R}^{V \\times d} $: Embedding matrix  </li> <li>$ x $: Input index  </li> </ul> </li> <li>Embedding function: $$ E(x) = W[x] $$</li> </ul> <pre><code># vocab_size: int = 102400\n# dim: int = 2048  # Model dimension.\n# s.embed = ParallelEmbedding(a.vocab_size, a.dim)\n\nclass ParallelEmbedding(nn.Module):\n    def __init__(s, vocab_size, dim):\n        super().__init__()\n        assert vocab_size % world_size == 0\n        s.dx = vocab_size // world_size\n        s.weight = nn.Parameter(tc.empty(s.dx, dim))\n\n    def forward(s, x: tc.Tensor):\n        x1 = rank * s.dx\n        if world_size &gt; 1:\n            mask = (x &lt; x1) | (x &gt;= x1 + s.dx)\n            x -= x1\n            x[mask] = 0\n        y = F.embedding(x, s.weight)\n        if world_size &gt; 1:\n            y[mask] = 0\n            dist.all_reduce(y)  # default op: sum\n        return y\n</code></pre>"},{"location":"DeepSeek.html#2-linear-layers","title":"2. Linear Layers","text":"<ul> <li> <p>torch.nn.functional.linear</p> </li> <li> <p>Maps a vector to another vector: $$ y = xW^T + b $$</p> </li> <li> <p>The code uses <code>quantization</code> + <code>parallelism</code>, I am ignoring these for now</p> <ul> <li><code>tc.float32</code> element size: 4 bytes</li> <li><code>tc.int8</code> element size: 1 byte</li> </ul> </li> </ul> <pre><code>def linear(x, w: tc.Tensor, b=None) -&gt; tc.Tensor:\n    if w.element_size() &gt; 1:\n        return F.linear(x, w, b)\n    elif gemm_impl == \"bf16\":\n        w = weight_dequant(w, w.scale)\n        return F.linear(x, w, b)\n    else:\n        x, scale = act_quant(x, block_size)\n        y = fp8_gemm(x, scale, w, w.scale)\n        return y if b is None else y + b\n</code></pre> <pre><code>class Linear(nn.Module):\n    part_out_features: int\n    dtype = tc.bfloat16\n\n    def __init__(s, I, O, bias=False, dtype=None):\n        super().__init__()\n        s.weight = nn.Parameter(tc.empty(O, I, dtype=dtype or Linear.dtype))\n        if s.weight.element_size() == 1:\n            O2 = (O + block_size - 1) // block_size\n            I2 = (I + block_size - 1) // block_size\n            s.weight.scale = s.scale = nn.Parameter(tc.empty(O2, I2, dtype=tc.float32))\n        else:\n            s.register_parameter(\"scale\", None)\n        if bias:\n            s.bias = nn.Parameter(tc.empty(s.part_out_features))\n        else:\n            s.register_parameter(\"bias\", None)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class ColumnParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert O % world_size == 0\n        s.part_out_features = O // world_size\n        super().__init__(I, s.part_out_features, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class RowParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert I % world_size == 0\n        s.part_in_features = I // world_size\n        super().__init__(s.part_in_features, O, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        y = linear(x, s.weight)\n        if world_size &gt; 1:\n            dist.all_reduce(y)\n        return y if s.bias is None else y + s.bias\n</code></pre>"},{"location":"DeepSeek.html#3-rms-normalization","title":"3. RMS Normalization","text":"<ul> <li> <p>torch.nn.RMSNorm</p> </li> <li> <p>Scales a vector (in order to have stabler gradients)</p> </li> </ul> \\[ y = \\frac{x}{\\mathrm{RMS}(x)} \\cdot \\gamma \\\\[5pt] \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{N} \\sum_i x_i^2} \\\\ \\gamma: \\text{learnable parameter} \\] <pre><code>class RMSNorm(nn.Module):\n    def __init__(s, dim, eps=1e-6):\n        super().__init__()\n        s.dim, s.eps = dim, eps\n        s.weight = nn.Parameter(tc.ones(dim))\n\n    def forward(s, x: tc.Tensor):\n        return F.rms_norm(x, (s.dim,), s.weight, s.eps)\n</code></pre>"},{"location":"DeepSeek.html#4-rope-rotary-position-embedding","title":"4. RoPE: Rotary Position Embedding","text":"<ul> <li>This transformation treats neural network activations as complex numbers, it applies complex rotations, encodes position $ t $ into vectors: </li> </ul> \\[  z' = z \\cdot e^{i \\omega t} \\\\[5pt] \\omega = {1 \\over \\text{base}^{d / D} } \\] <ul> <li>This ensures that the dot product (attention) after PE only depends on relative position \\(t_1 - t_2\\):</li> </ul> \\[ \\text{Re}(z_1 \\cdot z_2^*)  = \\text{Re}( (x_1 + i y_1) (x_2 - i y_2) )  = x_1 x_2 + y_1 y_2 =: \\text{Dot}(z_1, z_2) \\] \\[ \\text{Dot}(q', k') = \\text{Re}(q' \\cdot k'^*)  = \\text{Re}(q e^{i \\omega t_1} \\cdot k^* e^{-i \\omega t_2})  = \\text{Re}(q \\cdot k^* e^{i \\omega (t_1 - t_2)})  \\] <ul> <li>Below is my simple implementation:</li> </ul> <pre><code>def simple_RoPE(x: tc.Tensor, base=10000.0):\n    B, T, H, D2 = x.shape  # batch, time, head, dim*2\n    D = D2 // 2\n\n    t = tc.arange(T)  # shape: T\n    w = 1.0 / (base ** (tc.arange(0, D, dtype=tc.float32) / D))  # shape: D\n    wt = tc.outer(t, w)  # shape: T, D\n    e_iwt = tc.polar(tc.ones_like(wt), wt).view(1, T, 1, D)\n    z = tc.view_as_complex(x.float().view(B, T, H, D, 2))  # shape: B, T, H, D\n    y = tc.view_as_real(z * e_iwt).view(B, T, H, D2)\n    return y.to(x.dtype)\n</code></pre> full version <pre><code>def precompute_freqs_cis(a: ModelArgs):\n    dim = a.qk_rope_head_dim\n    base = a.rope_theta\n\n    def find_correction_dim(num_rot, dim, base, max_T):\n        return dim * math.log(max_T / (num_rot * 2 * math.pi)) / (2 * math.log(base))\n\n    def find_correction_range(low_rot, high_rot, dim, base, max_T):\n        low = math.floor(find_correction_dim(low_rot, dim, base, max_T))\n        high = math.ceil(find_correction_dim(high_rot, dim, base, max_T))\n        return max(low, 0), min(high, dim - 1)\n\n    def linear_ramp_factor(min, max, dim):\n        if min == max:\n            max += 0.001\n        linear_func = (tc.arange(dim, dtype=tc.float32) - min) / (max - min)\n        return tc.clamp(linear_func, 0, 1)\n\n    freqs = 1.0 / (base ** (tc.arange(0, dim, 2, dtype=tc.float32) / dim))\n    if a.max_seq_len &gt; a.original_seq_len:\n        low, high = find_correction_range(\n            a.beta_fast, a.beta_slow, dim, base, a.original_seq_len\n        )\n        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n        freqs = freqs / a.rope_factor * (1 - smooth) + freqs * smooth\n\n    t = tc.arange(a.max_seq_len)\n    freqs = tc.outer(t, freqs)\n    return tc.polar(tc.ones_like(freqs), freqs)\n\n\ndef apply_rotary_emb(x: tc.Tensor, freqs_cis: tc.Tensor):\n    dtype = x.dtype\n    x = tc.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n    return tc.view_as_real(x * freqs_cis).flatten(3).to(dtype)\n</code></pre>"},{"location":"DeepSeek.html#5-mla-multi-head-latent-attention","title":"5. MLA: Multi-head Latent Attention","text":"<ul> <li>Original Attention Mechanism<ul> <li>A weighted mixture of word meanings by combining the value vectors $ V $ using attention weights (similarity between queries $ Q $ and keys $ K $)</li> <li>$ n $: sequence length, $ d $: token embedding dim</li> </ul> </li> </ul> \\[ Q = X W_Q \\quad K = X W_K \\quad V = X W_V \\\\ A = \\text{softmax} \\left( {Q K^T \\over \\sqrt{d_k} } \\right) V \\\\ y_\\text{MultiHead} = \\text{Concat}(A_1, ..., A_h) \\; W_O \\] object shape $ X $ $ (n, d) $ $ W_Q, W_K, W_V $ $ (d, d_k) \\quad (d, d_k) \\quad (d, d_v) $ $ Q, K, V $ $ (n, d_k) \\quad (n, d_k) \\quad (n, d_v) $ $ Q K^T \\quad A $ $ (n, n) \\quad (n, d_v) $ $ W_O \\quad y $ $ (h \\cdot d_v, d) \\quad (n, d) $ <ul> <li> <p>LoRA: Low-Rank Adaptation</p> <ul> <li>Decompose $ W^{m\\times n} = W_B^{m\\times r} \\cdot W_A^{r\\times n} $ where $ r \\ll \\min(m, n) $ is the rank</li> <li>To reduce the number of parameters</li> <li>Essentially a compression $ (W_A) $ and decompression $ (W_B) $</li> <li>Latent space: the vector space after compression</li> </ul> </li> <li> <p>MLA</p> </li> </ul> \\[ q = W_{qB} \\cdot \\text{RMSNorm}(W_{qA} \\cdot x) \\text{ if LoRA else } W_q \\cdot x \\\\ \\text{split: } q \\rightarrow q_{\\text{nope}}, q_{\\text{pe}} \\rightarrow q_{\\text{nope}}, \\text{RoPE}( q_{\\text{pe}} ) \\rightarrow q \\\\[10pt]  kv, k_{\\text{pe}} = W_{kvA} \\cdot x \\\\  k_{\\text{pe}} = \\text{RoPE}( k_{\\text{pe}} ) \\\\  k_{\\text{nope}}, v = W_{kvB} \\cdot \\text{RMSNorm}(kv) \\\\ \\text{concat: } k_{\\text{nope}}, k_{\\text{pe}} \\rightarrow k \\\\[10pt] A = \\text{softmax} \\left( {Q K^T \\over \\sqrt{d_k} } \\right) V \\\\ y = W_O A \\] <p></p> <pre><code>class MLA(nn.Module):\n    k_cache: tc.Tensor\n    v_cache: tc.Tensor\n    kv_cache: tc.Tensor\n    pe_cache: tc.Tensor\n\n    def __init__(s, a: ModelArgs):\n        super().__init__()\n        s.args = a\n        s.n_local_heads = a.n_heads // world_size\n        s.qk_head_dim = a.qk_nope_head_dim + a.qk_rope_head_dim\n\n        if a.q_lora_rank == 0:\n            s.wq = ColumnParallelLinear(a.dim, a.n_heads * s.qk_head_dim)\n        else:\n            s.wq_a = Linear(a.dim, a.q_lora_rank)\n            s.q_norm = RMSNorm(a.q_lora_rank)\n            s.wq_b = ColumnParallelLinear(a.q_lora_rank, a.n_heads * s.qk_head_dim)\n        s.wkv_a = Linear(a.dim, a.kv_lora_rank + a.qk_rope_head_dim)\n        s.kv_norm = RMSNorm(a.kv_lora_rank)\n        s.wkv_b = ColumnParallelLinear(\n            a.kv_lora_rank, a.n_heads * (a.qk_nope_head_dim + a.v_head_dim)\n        )\n        s.wo = RowParallelLinear(a.n_heads * a.v_head_dim, a.dim)\n        s.softmax_scale = s.qk_head_dim**-0.5\n        if a.max_seq_len &gt; a.original_seq_len:\n            mscale = 0.1 * a.mscale * math.log(a.rope_factor) + 1.0\n            s.softmax_scale = s.softmax_scale * mscale * mscale\n\n        B, T, H = a.max_batch_size, a.max_seq_len, s.n_local_heads\n        persis = False\n        if attn_impl == \"naive\":\n            s.register_buffer(\"k_cache\", tc.zeros(B, T, H, s.qk_head_dim), persis)\n            s.register_buffer(\"v_cache\", tc.zeros(B, T, H, a.v_head_dim), persis)\n        else:\n            s.register_buffer(\"kv_cache\", tc.zeros(B, T, a.kv_lora_rank), persis)\n            s.register_buffer(\"pe_cache\", tc.zeros(B, T, a.qk_rope_head_dim), persis)\n\n    def forward(s, x: tc.Tensor, start_pos, freqs_cis, mask: tc.Tensor):\n        a = s.args\n        B, T, _ = x.size()\n        p1 = start_pos\n        p2 = p1 + T\n\n        if a.q_lora_rank == 0:\n            q: tc.Tensor = s.wq(x)\n        else:\n            q = s.wq_b(s.q_norm(s.wq_a(x)))\n\n        q = q.view(B, T, s.n_local_heads, s.qk_head_dim)\n        q_nope, q_pe = tc.split(q, [a.qk_nope_head_dim, a.qk_rope_head_dim], dim=-1)\n        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n\n        kv = s.wkv_a(x)\n        kv, k_pe = tc.split(kv, [a.kv_lora_rank, a.qk_rope_head_dim], dim=-1)\n        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n\n        if attn_impl == \"naive\":\n            q = tc.cat([q_nope, q_pe], dim=-1)\n            kv: tc.Tensor = s.wkv_b(s.kv_norm(kv))\n            kv = kv.view(B, T, s.n_local_heads, a.qk_nope_head_dim + a.v_head_dim)\n            k_nope, v = tc.split(kv, [a.qk_nope_head_dim, a.v_head_dim], dim=-1)\n            k = tc.cat([k_nope, k_pe.expand(-1, -1, s.n_local_heads, -1)], dim=-1)\n            s.k_cache[:B, p1:p2] = k\n            s.v_cache[:B, p1:p2] = v\n            scores: tc.Tensor = (\n                tc.einsum(\"bshd,bthd-&gt;bsht\", q, s.k_cache[:B, :p2]) * s.softmax_scale\n            )\n        else:\n            wkv_b = (\n                s.wkv_b.weight\n                if s.wkv_b.scale is None\n                else weight_dequant(s.wkv_b.weight, s.wkv_b.scale, block_size)\n            )\n            wkv_b = wkv_b.view(s.n_local_heads, -1, a.kv_lora_rank)\n            q_nope = tc.einsum(\"bshd,hdc-&gt;bshc\", q_nope, wkv_b[:, : a.qk_nope_head_dim])\n            s.kv_cache[:B, p1:p2] = s.kv_norm(kv)\n            s.pe_cache[:B, p1:p2] = k_pe.squeeze(2)\n            scores = (\n                tc.einsum(\"bshc,btc-&gt;bsht\", q_nope, s.kv_cache[:B, :p2])\n                + tc.einsum(\"bshr,btr-&gt;bsht\", q_pe, s.pe_cache[:B, :p2])\n            ) * s.softmax_scale\n        if mask is not None:\n            scores += mask.unsqueeze(1)\n        scores = scores.softmax(dim=-1, dtype=tc.float32).type_as(x)\n        if attn_impl == \"naive\":\n            x = tc.einsum(\"bsht,bthd-&gt;bshd\", scores, s.v_cache[:B, :p2])\n        else:\n            x = tc.einsum(\"bsht,btc-&gt;bshc\", scores, s.kv_cache[:B, :p2])\n            x = tc.einsum(\"bshc,hdc-&gt;bshd\", x, wkv_b[:, -a.v_head_dim :])\n        x = s.wo(x.flatten(2))\n        return x\n</code></pre>"},{"location":"DeepSeek.html#6-mlp-swiglu","title":"6. MLP (SwiGLU)","text":"<ul> <li>SiLU (Sigmoid Linear Unit) activation function.     Also called Swish function</li> </ul> \\[ \\text{SiLU}(x) = x \\cdot \\sigma(x) = { x \\over 1 + e^{-x} } \\] \\[ y = w_2( \\; \\text{SiLU}(w_1(x)) \\cdot w_3(x) \\; ) \\\\ w_i \\text{ : linear layers} \\] <ul> <li> <p>This is not a typical MLP.      It is a <code>SwiGLU</code> (Gated Linear Unit with Swish activation)</p> <ul> <li>gating mechanism (element-wise product) allows the model to selectively emphasize / suppress certain features. It outperform standard MLPs in many tasks</li> </ul> </li> <li> <p>This is used in DeepSeek as:</p> <ol> <li>Expert / Shared Experts</li> <li>Dense Feed-Forward Network (only used by the first transformer layer,     the rest use MoE: Mixture-of-Experts)</li> </ol> </li> </ul> <pre><code>class MLP(nn.Module):\n    def __init__(s, dim, inter_dim):\n        super().__init__()\n        s.w1 = ColumnParallelLinear(dim, inter_dim)\n        s.w2 = RowParallelLinear(inter_dim, dim)\n        s.w3 = ColumnParallelLinear(dim, inter_dim)\n\n    def forward(s, x):\n        return s.w2(F.silu(s.w1(x)) * s.w3(x))\n\nclass Expert(nn.Module):\n    def __init__(s, dim, inter_dim):\n        super().__init__()\n        s.w1 = Linear(dim, inter_dim)\n        s.w2 = Linear(inter_dim, dim)\n        s.w3 = Linear(dim, inter_dim)\n\n    def forward(s, x):\n        return s.w2(F.silu(s.w1(x)) * s.w3(x))\n\n# in MoE\n# s.experts.append(Expert(a.dim, a.moe_inter_dim) if s.i1 &lt;= i &lt; s.i2 else None)\n# s.shared_experts = MLP(a.dim, a.n_shared_experts * a.moe_inter_dim)\n\n# in Block\n# n_dense_layers: int = 1\n# s.ffn = MLP(a.dim, a.inter_dim) if layer_id &lt; a.n_dense_layers else MoE(a)\n</code></pre>"},{"location":"DeepSeek.html#7-moe-mixture-of-experts","title":"7. MoE: Mixture-of-Experts","text":"<ul> <li>Gate: expert selector for MoE.     Selects top-K experts (\"brain regions\") to use, to enhance efficiency</li> </ul> \\[ \\text{weights, indices} = \\text{TopK}(\\text{softmax}(\\text{Linear}(x))) \\\\[10pt] y = SE(x) + \\sum_i \\text{weights}_i \\cdot E_i (x) \\\\ SE \\text{ : shared experts} \\\\ E_i \\text{ : routed expert} \\] <pre><code>class Gate(nn.Module):\n    def __init__(s, a: ModelArgs):\n        super().__init__()\n        s.args = a\n        s.weight = nn.Parameter(tc.empty(a.n_routed_experts, a.dim))\n        s.bias = nn.Parameter(tc.empty(a.n_routed_experts)) if a.dim == 7168 else None\n\n    def forward(s, x: tc.Tensor):\n        a = s.args\n        scores = linear(x, s.weight)\n        if a.score_func == \"softmax\":\n            scores = scores.softmax(dim=-1, dtype=tc.float32)\n        else:\n            scores = scores.sigmoid()\n        original_scores = scores\n        if s.bias is not None:\n            scores = scores + s.bias\n        if a.n_expert_groups &gt; 1:\n            scores = scores.view(x.size(0), a.n_expert_groups, -1)\n            if s.bias is None:\n                group_scores = scores.amax(dim=-1)\n            else:\n                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n            indices = group_scores.topk(a.n_limited_groups, dim=-1)[1]\n            mask = tc.zeros_like(scores[..., 0]).scatter_(1, indices, True)\n            scores = (scores * mask.unsqueeze(-1)).flatten(1)\n        indices = tc.topk(scores, a.n_activated_experts, dim=-1)[1]\n        weights = original_scores.gather(1, indices)\n        if a.score_func == \"sigmoid\":\n            weights /= weights.sum(dim=-1, keepdim=True)\n        weights *= a.route_scale\n        return weights.type_as(x), indices\n\nclass MoE(nn.Module):\n    def __init__(s, a: ModelArgs):\n        super().__init__()\n        s.args = a\n        assert a.n_routed_experts % world_size == 0\n        s.n_local_experts = a.n_routed_experts // world_size\n        s.i1 = rank * s.n_local_experts\n        s.i2 = s.i1 + s.n_local_experts\n        s.gate = Gate(a)\n        s.experts = nn.ModuleList()\n        for i in range(a.n_routed_experts):\n            s.experts.append(\n                Expert(a.dim, a.moe_inter_dim) if s.i1 &lt;= i &lt; s.i2 else None\n            )\n        s.shared_experts = MLP(a.dim, a.n_shared_experts * a.moe_inter_dim)\n\n    def forward(s, x: tc.Tensor):\n        a = s.args\n        shape = x.size()\n        x = x.view(-1, a.dim)\n        weights, indices = s.gate(x)\n        y = tc.zeros_like(x)\n        counts = tc.bincount(indices.flatten(), minlength=a.n_routed_experts).tolist()\n        for i in range(s.i1, s.i2):\n            if counts[i] == 0:\n                continue\n            expert = s.experts[i]\n            idx, top = tc.where(indices == i)\n            y[idx] += expert(x[idx]) * weights[idx, top, None]\n        z = s.shared_experts(x)\n        if world_size &gt; 1:\n            dist.all_reduce(y)\n        return (y + z).view(shape)\n</code></pre>"},{"location":"DeepSeek.html#8-transformer","title":"8. Transformer","text":"<ul> <li>simply combines the above modules</li> </ul> \\[ x \\text{ : input tokens} \\\\ x \\leftarrow \\text{Embedding}(x) \\\\[10pt] \\text{(transformer layers:)} \\\\ x \\leftarrow x + \\text{MLA}(\\text{RMSNorm}(x)) \\\\ x \\leftarrow x + \\text{FFN}(\\text{RMSNorm}(x)) \\\\ \\text{FFN : MLP or MoE} \\\\[10pt] y_\\text{logits} = \\text{Linear}(\\text{RMSNorm}(x)) \\] <pre><code>class Block(nn.Module):\n    def __init__(s, layer_id, a: ModelArgs):\n        super().__init__()\n        s.attn = MLA(a)\n        s.ffn = MLP(a.dim, a.inter_dim) if layer_id &lt; a.n_dense_layers else MoE(a)\n        s.attn_norm = RMSNorm(a.dim)\n        s.ffn_norm = RMSNorm(a.dim)\n\n    def forward(s, x: tc.Tensor, start_pos, freqs_cis, mask):\n        x = x + s.attn(s.attn_norm(x), start_pos, freqs_cis, mask)\n        x = x + s.ffn(s.ffn_norm(x))\n        return x\n\nclass Transformer(nn.Module):\n    freqs_cis: tc.Tensor\n\n    def __init__(s, a: ModelArgs):\n        global world_size, rank\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        Linear.dtype = tc.float8_e4m3fn if a.dtype == \"fp8\" else tc.bfloat16\n        super().__init__()\n        s.embed = ParallelEmbedding(a.vocab_size, a.dim)\n        s.layers = tc.nn.ModuleList([Block(i, a) for i in range(a.n_layers)])\n        s.norm = RMSNorm(a.dim)\n        s.head = ColumnParallelLinear(a.dim, a.vocab_size, dtype=tc.get_default_dtype())\n        s.register_buffer(\"freqs_cis\", precompute_freqs_cis(a), persistent=False)\n\n    @tc.inference_mode()\n    def forward(s, tokens: tc.Tensor, start_pos=0):\n        T = tokens.size(1)\n        h = s.embed(tokens)\n        freqs_cis = s.freqs_cis[start_pos : start_pos + T]\n        mask = None\n        if T &gt; 1:\n            mask = tc.full((T, T), float(\"-inf\"), device=tokens.device).triu_(1)\n        for layer in s.layers:\n            h = layer(h, start_pos, freqs_cis, mask)\n        h = s.norm(h)[:, -1]\n        logits = s.head(h)\n        if world_size &gt; 1:\n            all_logits = [tc.empty_like(logits) for _ in range(world_size)]\n            dist.all_gather(all_logits, logits)\n            logits = tc.cat(all_logits, dim=-1)\n        return logits\n</code></pre>"},{"location":"RL.html","title":"RL","text":""},{"location":"RL.html#summary-of-deep-reinforcement-learning","title":"Summary of Deep Reinforcement Learning","text":"<ul> <li>CS285 at UC Berkeley<ul> <li>videos</li> </ul> </li> </ul>"},{"location":"RL.html#lecture-1-intro-and-overview","title":"Lecture 1: Intro and overview","text":""},{"location":"RL.html#beyond-learning-from-reward","title":"Beyond learning from reward","text":"<ul> <li>basic RL: maximize rewards</li> <li>other methods:<ul> <li><code>Inverse RL</code>: learn reward function from example</li> <li><code>Transfer Learning</code>: transfer knowledge between domains</li> <li><code>Meta learning</code>: learning to learn</li> <li><code>Predicting</code>: use predictions to act</li> </ul> </li> </ul>"},{"location":"RL.html#lecture-2-supervised-learning-of-behaviors","title":"Lecture 2: Supervised Learning of behaviors","text":"<ul> <li>small deviations accumulate to very different trajectories and states compared with training data (not Markovian)</li> <li>solution: generate examples of \"mistakes\" and their \"corrections\" (teach \"what didn't work and how to fix\", not \"what worked\")</li> </ul>"},{"location":"RL.html#lecture-3-pytorch-tutorial","title":"Lecture 3: PyTorch tutorial","text":""},{"location":"RL.html#lecture-4-intro-to-rl","title":"Lecture 4: Intro to RL","text":"<ul> <li>expanding the total reward over trajectory $ \\tau = (s_1, a_1, s_2, a_2 \\ldots) $:</li> </ul> \\[ \\begin{align*}     J &amp;= E_\\tau \\underbrace{ \\sum_t r(s_t, a_t) }_{ r(\\tau) }  \\\\     &amp;= E_{s_1} \\underbrace{         E_{a_1} \\underbrace{             r(s_1, a_1) + E_{s_2} E_{a_2} r(s_2, a_2) + \\ldots         }_{ Q(s_1, a_1) }     }_{V(s_1)}  \\end{align*} \\]"},{"location":"RL.html#types-of-rl-algorithms","title":"Types of RL algorithms","text":"<ul> <li><code>off policy</code>: able to improve the policy without generating new samples from that policy</li> <li><code>on policy</code>: once the policy is changed, need to generate new samples</li> </ul> <ul> <li><code>Value function fitting</code>: At best, minimizes error of fit (Bellman error), not the same as expected reward; At worst, doesn't optimize anything.</li> <li><code>Model based</code>: minimizes error of fit, but better model != better policy</li> <li><code>Policy Gradient</code>: gradient descent on true objective</li> </ul>"},{"location":"RL.html#lecture-5-policy-gradients","title":"Lecture 5: Policy Gradients","text":"\\[ \\begin{align*} J(\\theta) &amp;= E_\\tau r(\\tau) = \\int p_\\theta(\\tau) \\; r(\\tau) \\; d\\tau  \\\\ \\nabla_\\theta J &amp;= \\int \\nabla p \\; r(\\tau) \\; d\\tau =  \\int p \\nabla \\log p \\; r(\\tau) \\; d\\tau  =  E_\\tau \\underbrace{ \\nabla \\log p }_{ \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) } \\; r(\\tau) \\\\ \\text{because} \\; p &amp;= p(s_1) \\prod_t \\pi_\\theta(a_t | s_t) \\; p(s_{t+1} | s_t, a_t) \\end{align*} \\] <p>notation $ \\nabla \\log \\pi(\\tau) := \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) $ </p> <ul> <li>approximate with sample mean:</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\left( \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t}) \\right)  \\left( \\sum_{t=1}^T r(s_{n, t}, a_{n, t}) \\right) \\]"},{"location":"RL.html#improvement-1-reward-to-go","title":"<code>Improvement 1</code>: reward to go","text":"<ul> <li>causality: actions only affect the future, remove past rewards</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t})  \\underbrace{ \\left( \\sum_{t'=t}^T r(s_{n, t'}, a_{n, t'}) \\right) }_{ \\text{reward to go} \\; \\hat Q_{n, t} }  \\]"},{"location":"RL.html#improvement-2-subtracting-a-reward-baseline","title":"<code>Improvement 2</code>: subtracting a reward baseline","text":"<ul> <li>a simple baseline: mean return</li> </ul> \\[ \\begin{align*} \\nabla J &amp; \\approx {1\\over N} \\sum_n \\nabla \\log p \\; [r(\\tau) - b] \\\\ b &amp; := {1\\over N} \\sum_n r(\\tau) \\\\  \\text{because} \\; E [\\nabla \\log p \\; b] &amp;= \\int \\underbrace{ p \\nabla \\log p }_{ \\nabla p } \\; b \\; d\\tau = b \\nabla \\underbrace{ \\int p \\; d\\tau }_{ 1 } = 0 \\end{align*} \\] <ul> <li>optimal baseline</li> </ul> \\[ \\begin{align*} \\text{Var} [x] &amp;= E[x^2] - E[x]^2 \\\\ \\nabla J &amp;= E[ \\underbrace{ \\nabla \\log p }_{g} \\; (r - b) ] \\\\ \\text{Var} &amp;= E[ (g (r-b))^2 ] - \\underbrace{ E[ g (r - b) ]^2 }_{ = E[ g r ]^2 } \\\\ \\text{let} &amp; \\; {d \\text{Var} \\over db} = 0 \\\\  \\implies &amp; {d \\over db} \\left( -2b E[g(\\tau)^2 r(\\tau)] + b^2 E[g(\\tau)^2] \\right) = 0 \\\\ \\implies &amp; b = { E[g^2 r] \\over E[g^2] } \\end{align*} \\]"},{"location":"RL.html#improvement-3-from-on-policy-to-off-policy-pg","title":"<code>Improvement 3</code>: from on-policy to off-policy PG","text":"<p>the above result is on-policy \u2014\u2014 need to generate new samples whenever policy neural net is updated</p> <ul> <li>importance sampling: learn about one distribution from another distribution</li> </ul> \\[ E_{x\\sim p(x)}[y] = \\int p(x) \\; y \\; dx = \\int q(x) {p(x) \\over q(x)} \\; y \\; dx = E_{x\\sim q(x)}[ {p(x) \\over q(x)} y] \\] <ul> <li>didn't quite understand pages 24-26, conclusion:</li> </ul> \\[ \\begin{align*} \\text{on-policy } &amp; \\nabla_\\theta J(\\theta) \\approx {1\\over N} \\sum_n \\sum_t \\nabla_\\theta \\log \\pi_\\theta (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\\\ \\text{off-policy } &amp; \\nabla_\\alpha J(\\alpha) \\approx {1\\over N} \\sum_n \\sum_t {\\pi_\\alpha(a_{n, t} | s_{n, t}) \\over \\pi_\\theta (a_{n, t} | s_{n, t}) } \\nabla_\\alpha \\log \\pi_\\alpha (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\end{align*} \\]"},{"location":"RL.html#improvement-4-natural-pg-rescaling-the-vanilla-pg","title":"<code>Improvement 4</code>: Natural PG: rescaling the Vanilla PG","text":"<ul> <li>didn't quite understand pages 35-36, conclusion:</li> </ul> \\[ \\begin{align*} \\text{ Vanilla: } &amp; \\theta \\leftarrow \\theta + \\alpha \\nabla J \\\\ \\text{ Natural: } &amp; \\theta \\leftarrow \\theta + \\alpha F^{-1} \\nabla J \\\\ \\text{ where: } &amp; F = E_{\\pi_\\theta} [ (\\nabla \\log \\pi) \\; (\\nabla \\log \\pi)^T ] \\end{align*} \\]"},{"location":"RL.html#lecture-6-actor-critic","title":"Lecture 6: Actor-Critic","text":""},{"location":"RL.html#improvement-5-actor-critic-still-trying-to-reduce-variance","title":"<code>Improvement 5</code>: Actor-Critic: still trying to reduce variance","text":"\\[ \\begin{align*} \\text{before: single trajectory } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\hat Q_{n, t} - b) \\\\ \\text{AC: Exp over trajectories } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\underbrace{ Q(s_{n, t}, a_{n, t}) - V(s_{n, t}) }_{ A: \\text{ advantage } } ) \\end{align*} \\] \\[ \\begin{align*} Q(s_t, a_t) &amp;= r(s_t, a_t) + E_{ s_{t+1} } [V(s_{t+1})] \\\\ &amp;\\approx r(s_t, a_t) + V(s_{t+1}) \\\\ \\implies A(s_t, a_t) &amp;\\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t) \\end{align*} \\] <ul> <li>use neural net \\(\\phi\\) to approximate \\(V\\) (supervised learning)</li> </ul> \\[ \\begin{align*} L_{\\text{MSE}}(\\phi) &amp;= {1\\over 2} \\sum_n \\lVert V_\\phi(s_n) - y_n \\rVert^2 \\\\ y_{n, t} &amp;= r(s_{n, t}, a_{n, t}) + \\underbrace{ \\gamma \\in [0, 1] }_{ \\text{ discount } } \\; \\underbrace{ V_\\phi(s_{n, \\; t+1}) }_{ \\text{ bootstrapping } }  \\end{align*} \\] <ul> <li>skip pages 14-15: discount factor for PG</li> </ul>"},{"location":"RL.html#improvement-6-from-on-policy-to-off-policy-ac","title":"<code>Improvement 6</code>: from on-policy to off-policy AC","text":"<ul> <li>on-policy AC</li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r) \\\\ &amp; \\text{2. fit } V_\\phi \\text{ using } r + \\gamma V_\\phi(s') \\\\ &amp; \\text{3. eval } A(s, a) = r(s, a) + \\gamma V_\\phi(s') - V_\\phi(s) \\\\ &amp; \\text{4. } \\nabla J \\approx \\nabla \\log \\pi(a | s) A(s, a) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <ul> <li>off-policy AC, can still use importance sampling, but use another method: $ V \\to Q $<ul> <li>cannot use the above directly because: $ a $ comes from old policies, not latest</li> </ul> </li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r), \\text{ store in } R \\\\ &amp; \\text{2. sample a batch } \\{ s_n, a_n, s_n', r_n \\} \\text{ from } R \\\\ &amp; \\text{3. fit } Q_\\phi \\text{ using } y_n = r_n + \\gamma Q_\\phi(s_n', a_n') \\text{ for each } s_n, a_n \\quad a_n' \\sim \\pi(a | s_n') \\\\ &amp; \\text{4. } \\nabla J \\approx {1\\over N} \\sum_n \\nabla \\log \\pi(a_n^\\pi | s_n) Q(s_n, a_n^\\pi) \\quad a_n^\\pi \\sim \\pi(a|s_n) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <p>page 26</p>"},{"location":"NeuroAI/2017_NeuroAI_1.html","title":"2017 NeuroAI 1","text":"<p>Generated by DeepSeek</p>"},{"location":"NeuroAI/2017_NeuroAI_1.html#neuroscience-inspired-artificial-intelligence","title":"Neuroscience-Inspired Artificial Intelligence","text":"<ul> <li>2017, Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, Matthew Botvinick</li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#summary","title":"Summary","text":"<ul> <li> <p>Past: Deep Learning</p> <ul> <li>Origins in Neuroscience<ul> <li>ANN (1940s): Inspired by logical functions (McCulloch and Pitts, 1943)</li> <li>Learning: Supervised (Rosenblatt, 1958), Unsupervised (Hebb, 1949)</li> </ul> </li> <li>Backpropagation &amp; PDP<ul> <li>Backpropagation: Multi-layered learning (Rumelhart et al., 1985; Werbos, 1974)</li> <li>PDP: Distributed cognition, error/reward optimization (Rumelhart et al., 1986)</li> </ul> </li> <li>PDP Influence<ul> <li>Distributed representations: Vectors for words/sentences (St. John and McClelland, 1990)</li> <li>Models human behaviors (Hinton et al., 1986)</li> </ul> </li> <li>CNNs<ul> <li>Features: Nonlinear transduction, divisive normalization, max-pooling (Hubel and Wiesel, 1959)</li> <li>Hierarchical organization (Fukushima, 1980; LeCun et al., 1989; Krizhevsky et al., 2012)</li> </ul> </li> <li>Advancements<ul> <li>Deep Belief Networks (Hinton et al., 2006)</li> <li>Large datasets (Deng et al., 2009)</li> <li>Dropout: Stochastic regularization (Hinton et al., 2012)</li> </ul> </li> </ul> </li> <li> <p>Past: Reinforcement Learning (RL)</p> <ul> <li>Origins<ul> <li>Animal conditioning (Sutton and Barto, 1981)</li> <li>TD Learning: Second-order conditioning (Sutton and Barto, 1998)</li> </ul> </li> <li>Impact<ul> <li>Maps states to actions (Sutton and Barto, 1998)</li> <li>Applications: Robotics (Hafner and Riedmiller, 2011), Backgammon (Tesauro, 1995), Go (Silver et al., 2016)</li> </ul> </li> </ul> </li> <li> <p>Present: Attention</p> <ul> <li>Modular Brain Structure<ul> <li>Subsystems: Memory, language, cognitive control (Anderson et al., 2004)</li> </ul> </li> <li>Visual Attention<ul> <li>Sequential processing (Koch and Ullman, 1985)</li> <li>Neurocomputational models (Olshausen et al., 1993)</li> </ul> </li> <li>AI Architectures<ul> <li>Sequential glimpses (Larochelle and Hinton, 2010; Mnih et al., 2014)</li> <li>Selective attention: Cluttered classification (Mnih et al., 2014)</li> <li>Multi-object recognition (Ba et al., 2015)</li> <li>Image-to-caption (Xu et al., 2015)</li> </ul> </li> <li>Internal Attention<ul> <li>Memory focus (Summerfield et al., 2006)</li> <li>Machine translation (Bahdanau et al., 2014)</li> <li>Content-addressable retrieval (Hopfield, 1982)</li> </ul> </li> <li>Generative Models<ul> <li>DRAW: Incremental image synthesis (Gregor et al., 2015)</li> </ul> </li> </ul> </li> <li> <p>Present: Episodic Memory</p> <ul> <li>Neuroscience Basis<ul> <li>Episodic memory: Hippocampal one-shot encoding (Tulving, 1985)</li> </ul> </li> <li>DQN<ul> <li>Experience replay: Hippocampal-neocortical interaction (Mnih et al., 2015)</li> <li>Reward-based replay (Schaul et al., 2015)</li> </ul> </li> <li>Complementary Learning<ul> <li>Hippocampal rapid encoding, neocortical consolidation (O\u2019Neill et al., 2010)</li> </ul> </li> <li>Episodic Control<ul> <li>Rapid behavioral changes (Blundell et al., 2016)</li> <li>One-shot learning (Vinyals et al., 2016)</li> </ul> </li> </ul> </li> <li> <p>Present: Working Memory</p> <ul> <li>Human Basis<ul> <li>Prefrontal cortex, central executive (Baddeley, 2012)</li> </ul> </li> <li>RNN<ul> <li>Attractor dynamics (Elman, 1990)</li> </ul> </li> <li>LSTM<ul> <li>Gated memory maintenance (Hochreiter and Schmidhuber, 1997)</li> <li>Query response (Zaremba and Sutskever, 2014)</li> </ul> </li> <li>DNC<ul> <li>External memory matrix (Graves et al., 2016)</li> <li>Complex reasoning (Graves et al., 2014)</li> </ul> </li> </ul> </li> <li> <p>Present: Continual Learning</p> <ul> <li>Challenges<ul> <li>Catastrophic forgetting (French, 1999)</li> </ul> </li> <li>Neuroscience Insights<ul> <li>Dendritic spine dynamics (Nishiyama and Yasuda, 2015)</li> <li>Synaptic protection (Cichon and Gan, 2015)</li> </ul> </li> <li>AI Solutions<ul> <li>EWC: Weight consolidation (Kirkpatrick et al., 2017)</li> </ul> </li> </ul> </li> <li> <p>Future: Achievements</p> <ul> <li>Object recognition (Krizhevsky et al., 2012)</li> <li>Games: Atari (Mnih et al., 2015), Go (Silver et al., 2016), Poker (Morav\u010d\u00edk et al., 2017)</li> <li>Image/speech synthesis (Lake et al., 2015)</li> <li>Multilingual translation (Wu et al., 2016)</li> <li>Neural art (Gatys et al., 2015)</li> </ul> </li> <li> <p>Future: Intuitive Understanding</p> <ul> <li>Human Cognition<ul> <li>Innate physical concepts (Spelke and Kinzler, 2007)</li> </ul> </li> <li>AI Approaches<ul> <li>Scene decomposition (Battaglia et al., 2016)</li> <li>Deep RL commonsense (Denil et al., 2016)</li> <li>Deep generative models: Disentangled representations (Higgins et al., 2016)</li> </ul> </li> </ul> </li> <li> <p>Future: Efficient Learning</p> <ul> <li>Human Efficiency<ul> <li>Few-shot learning (Lake et al., 2016)</li> </ul> </li> <li>AI Progress<ul> <li>Probabilistic models (Lake et al., 2015)</li> <li>DRAW-based one-shot learning (Rezende et al., 2016b)</li> <li>Meta-learning (Santoro et al., 2016)</li> </ul> </li> </ul> </li> <li> <p>Future: Transfer Learning</p> <ul> <li>AI Architectures<ul> <li>Progressive Networks: Knowledge transfer (Rusu et al., 2016a)</li> <li>Compositional representations (Higgins et al., 2016)</li> </ul> </li> <li>Neuroscience Basis<ul> <li>Grid codes: Abstract reasoning (Constantinescu et al., 2016)</li> </ul> </li> </ul> </li> <li> <p>Future: Imagination &amp; Planning</p> <ul> <li>Limitations<ul> <li>Model-free RL inefficiency (Daw et al., 2005)</li> </ul> </li> <li>Human/Animal Planning<ul> <li>Simulation-based (Daw et al., 2005)</li> <li>Hippocampal preplay (Johnson and Redish, 2007)</li> </ul> </li> <li>AI Techniques<ul> <li>Dyna: Hypothetical experiences (Sutton, 1991)</li> <li>Model-based RL, MCTS (Silver et al., 2016)</li> <li>Deep generative models: Coherent sequences (Eslami et al., 2016)</li> </ul> </li> </ul> </li> <li> <p>Future: Virtual Brain Analytics</p> <ul> <li>Neuroscience Tools<ul> <li>Dimensionality reduction (Zahavy et al., 2016)</li> <li>Receptive field mapping (Nguyen et al., 2016)</li> <li>Linearized network analysis (Saxe et al., 2013)</li> </ul> </li> </ul> </li> <li> <p>From AI to Neuroscience</p> <ul> <li>Applications<ul> <li>Neuroimaging: fMRI/MEG analysis (Cichy et al., 2014)</li> <li>RL: TD prediction errors (Schultz et al., 1997)</li> <li>CNN: Ventral visual stream (Yamins and DiCarlo, 2016)</li> <li>LSTM: Prefrontal gating (O\u2019Reilly and Frank, 2006)</li> <li>Memory-augmented networks: Hippocampal querying (Kumaran and McClelland, 2012)</li> <li>Meta-RL: Prefrontal RL (Duan et al., 2016)</li> <li>Backpropagation: Biologically plausible approximations (Lillicrap et al., 2016)</li> </ul> </li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#past-deep-learning","title":"Past: Deep Learning","text":"<ul> <li> <p>Origins in Neuroscience:</p> <ul> <li>ANN, inspired by neuroscience, were developed in the 1940s to compute logical functions (McCulloch and Pitts, 1943).</li> <li>Learning mechanisms proposed include supervised incremental learning (Rosenblatt, 1958) and unsupervised encoding of environmental statistics (Hebb, 1949).</li> </ul> </li> <li> <p>Backpropagation and Parallel Distributed Processing (PDP):</p> <ul> <li>Backpropagation algorithm enabled learning in multi-layered networks (Rumelhart et al., 1985; Werbos, 1974).</li> <li>PDP movement, led by neuroscientists, proposed cognition emerges from distributed interactions in neuron-like units, tuned by learning to minimize error or maximize reward (Rumelhart et al., 1986).</li> <li>Contrasted with symbolic AI, PDP emphasized stochastic, parallelized processing inspired by brain function.</li> </ul> </li> <li> <p>PDP Influence on AI:</p> <ul> <li>PDP models introduced distributed representations (e.g., vectors for words/sentences), foundational for modern machine translation (St. John and McClelland, 1990; LeCun et al., 2015).</li> <li>Successfully modeled diverse human behaviors using small-scale problems (Hinton et al., 1986).</li> </ul> </li> <li> <p>CNNs:</p> <ul> <li>CNNs incorporate neuroscience-inspired features: nonlinear transduction, divisive normalization, and max-pooling, derived from V1 simple and complex cell recordings (Hubel and Wiesel, 1959; Yamins and DiCarlo, 2016).</li> <li>Replicate hierarchical cortical organization with convergent/divergent information flow (Fukushima, 1980; LeCun et al., 1989; Krizhevsky et al., 2012).</li> <li>Successive non-linear computations enable invariant object recognition across pose, illumination, and scale.</li> </ul> </li> <li> <p>Advancements in Deep Learning:</p> <ul> <li>Deep belief networks advanced NN capabilities (Hinton et al., 2006).</li> <li>Large datasets, inspired by human language research, improved training (Deng et al., 2009).</li> <li>Dropout regularization, motivated by stochastic neuronal firing (Poisson-like statistics), enhances generalization (Hinton et al., 2012).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#past-reinforcement-learning","title":"Past: Reinforcement Learning","text":"<ul> <li> <p>Origins in Neuroscience:</p> <ul> <li>RL emerged from studies of animal learning, particularly conditioning experiments (Sutton and Barto, 1981).</li> <li>Temporal-difference (TD) methods, a core RL component, were inspired by animal behavior in second-order conditioning, where a conditioned stimulus gains significance through association with another conditioned stimulus.</li> </ul> </li> <li> <p>TD Learning Mechanism:</p> <ul> <li>TD methods learn from differences between successive predictions in real-time, without waiting for actual rewards.</li> <li>Provides a natural explanation for second-order conditioning and broader neuroscience findings.</li> </ul> </li> <li> <p>Impact on AI:</p> <ul> <li>RL maps environmental states to actions to maximize future rewards, widely used in AI research (Sutton and Barto, 1998).</li> <li>TD methods and related techniques enabled advances in robotic control (Hafner and Riedmiller, 2011), expert backgammon play (Tesauro, 1995), and Go (Silver et al., 2016).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#present-attention","title":"Present: Attention","text":"<ul> <li> <p>Modular Brain Structure:</p> <ul> <li>Biological brains are modular, with distinct subsystems for memory, language, and cognitive control (Anderson et al., 2004; Shallice, 1988).</li> <li>This modularity influences AI architectures, incorporating specialized subsystems.</li> </ul> </li> <li> <p>Visual Attention in Primates:</p> <ul> <li>Primate visual systems prioritize processing through strategic shifts of attention, focusing on specific regions sequentially (Koch and Ullman, 1985; Moore and Zir imperfections, 2017; Posner and Petersen, 1990).</li> <li>Neurocomputational models show this approach isolates relevant information, enhancing behavioral outcomes (Olshausen et al., 1993; Salinas and Abbott, 1997).</li> </ul> </li> <li> <p>Attention in AI Architectures:</p> <ul> <li>AI models mimic visual attention by taking sequential \"glimpses\" of input images, updating internal states, and selecting subsequent sampling locations (Larochelle and Hinton, 2010; Mnih et al., 2014).</li> <li>Selective attention enables networks to ignore irrelevant objects, improving performance in cluttered object classification tasks (Mnih et al., 2014).</li> <li>Attention reduces computational cost, scaling efficiently with input image size (Mnih et al., 2014).</li> <li>Extended attention mechanisms outperform conventional CNNs in multi-object recognition, improving accuracy and efficiency (Ba et al., 2015).</li> <li>Attention enhances image-to-caption generation (Xu et al., 2015).</li> </ul> </li> <li> <p>Internal Attention and Memory:</p> <ul> <li>Attention can focus on internal memory contents, inspired by neuroscience (Summerfield et al., 2006).</li> <li>AI architectures use attention to selectively read from internal memory, advancing machine translation (Bahdanau et al., 2014) and memory/reasoning tasks (Graves et al., 2016).</li> <li>Implements content-addressable retrieval, a concept from neuroscience (Hopfield, 1982).</li> </ul> </li> <li> <p>Attention in Generative Models:</p> <ul> <li>Deep generative models incorporate attention to synthesize realistic images (Hong et al., 2015; Reed et al., 2016).</li> <li>The DRAW model uses attention to incrementally build images by focusing on portions of a \"mental canvas\" (Gregor et al., 2015).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#present-episodic-memory","title":"Present: Episodic Memory","text":"<ul> <li> <p>Multiple Memory Systems in Neuroscience:</p> <ul> <li>Intelligent behavior relies on reinforcement-based and instance-based (episodic) memory systems (Tulving, 1985).</li> <li>Episodic memory enables rapid, one-shot encoding of experiences in a content-addressable store, primarily in the hippocampus (Squire et al., 2004; Tulving, 2002).</li> </ul> </li> <li> <p>Experience Replay in Deep Q-Networks (DQN):</p> <ul> <li>DQN integrates RL with deep learning, achieving expert-level performance in Atari 2600 games by transforming image pixels into action policies (Mnih et al., 2015; Silver et al., 2016).</li> <li>Experience replay, inspired by hippocampal-neocortical interactions, stores and replays training data offline to enhance data efficiency and prevent destabilization from correlated experiences (Kumaran et al., 2016; McClelland et al., 1995).</li> <li>Replay of highly rewarding events improves DQN performance, mirroring hippocampal replay of rewarding experiences (Schaul et al., 2015; Singer and Frank, 2009).</li> </ul> </li> <li> <p>Hippocampal-Neocortical Complementary Learning:</p> <ul> <li>The hippocampus encodes novel information rapidly, which is consolidated to the neocortex during sleep or rest via replay of neural activity patterns (O\u2019Neill et al., 2010; Skaggs and McNaughton, 1996).</li> <li>This mechanism prevents catastrophic forgetting in NN by mitigating interference from sequential tasks (McClelland et al., 1995).</li> </ul> </li> <li> <p>Episodic Control in AI:</p> <ul> <li>Episodic control, inspired by hippocampal function, enables rapid behavioral changes by re-enacting rewarded action sequences from memory (Gershman and Daw, 2017).</li> <li>AI architectures implementing episodic control store experiences (e.g., actions, rewards, game states) and select actions based on similarity to past events, improving early learning performance (Blundell et al., 2016).</li> <li>Episodic control excels in one-shot learning tasks where traditional deep RL struggles (Blundell et al., 2016).</li> </ul> </li> <li> <p>Broader Episodic-Like Memory Systems:</p> <ul> <li>Episodic-like memory enables rapid learning of new concepts from few examples (Vinyals et al., 2016).</li> <li>Future architectures may integrate rapid episodic memory with incremental learning, mirroring complementary learning systems in the mammalian brain.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#present-working-memory","title":"Present: Working Memory","text":"<ul> <li> <p>Human Working Memory:</p> <ul> <li>Maintains and manipulates information in an active store, primarily in the prefrontal cortex and interconnected areas (Goldman-Rakic, 1990).</li> <li>Classic models propose a central executive interacting with domain-specific buffers (e.g., visuo-spatial sketchpad) (Baddeley, 2012).</li> </ul> </li> <li> <p>RNN:</p> <ul> <li>Inspired by neuroscience, early RNNs with attractor dynamics model sequential behavior and working memory (Elman, 1990; Hopfield and Tank, 1986; Jordan, 1997).</li> <li>Enable detailed modeling of human working memory (Botvinick and Plaut, 2006; Durstewitz et al., 2000).</li> </ul> </li> <li> <p>Long Short-Term Memory (LSTM) Networks:</p> <ul> <li>Evolved from RNNs, LSTMs gate information into a fixed activity state for maintenance and retrieval (Hochreiter and Schmidhuber, 1997).</li> <li>Achieve state-of-the-art performance in tasks like responding to queries about latent variable states after training on computer code (Zaremba and Sutskever, 2014).</li> <li>Intertwine sequence control and memory storage, unlike separate modules in classic human working memory models.</li> </ul> </li> <li> <p>Differential Neural Computer (DNC):</p> <ul> <li>Inspired by neuroscience\u2019s separation of control and storage, DNC uses a NN controller to read/write to an external memory matrix (Graves et al., 2016).</li> <li>Performs complex memory and reasoning tasks (e.g., finding shortest paths in graphs, manipulating blocks in Tower of Hanoi) via end-to-end optimization (Graves et al., 2014, 2016; Weston et al., 2014).</li> <li>Overcomes limitations of traditional NN, previously thought to require symbol processing and variable binding (Fodor and Pylyshyn, 1988; Marcus, 1998).</li> </ul> </li> <li> <p>Long-Term Memory Potential:</p> <ul> <li>LSTMs and DNC can retain information over thousands of training cycles, suitable for long-term memory tasks like understanding book contents.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#present-continual-learning","title":"Present: Continual Learning","text":"<ul> <li> <p>Continual Learning in Biological and Artificial Agents:</p> <ul> <li>Both biological and artificial agents require continual learning to master new tasks without forgetting prior ones (Thrun and Mitchell, 1995).</li> <li>NN suffer from catastrophic forgetting, where learning a new task overwrites parameters needed for previous tasks (French, 1999; McClelland et al., 1995).</li> </ul> </li> <li> <p>Neuroscience Insights on Continual Learning:</p> <ul> <li>Advanced neuroimaging (e.g., two-photon imaging) enables visualization of dendritic spine dynamics during learning at single-synapse resolution (Nishiyama and Yasuda, 2015).</li> <li>Neocortical plasticity studies reveal mechanisms protecting prior task knowledge, including reduced synaptic lability via persistent dendritic spine enlargements (Cichon and Gan, 2015; Yang et al., 2009).</li> <li>These structural changes correlate with task retention over months; erasing them with synaptic optogenetics causes forgetting (Hayashi-Takagi et al., 2015).</li> <li>Theoretical models suggest memories are protected by synapses transitioning through states with varying plasticity levels (Fusi et al., 2005).</li> </ul> </li> <li> <p>AI Developments Inspired by Neuroscience:</p> <ul> <li>Elastic Weight Consolidation (EWC) algorithm, inspired by neuroscience, slows learning in weights critical to prior tasks, anchoring them to previous solutions (Kirkpatrick et al., 2017).</li> <li>EWC enables deep RL networks to support continual learning without expanding network capacity, efficiently sharing weights across related tasks.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#future","title":"Future","text":"<ul> <li> <p>AI Performance Achievements:</p> <ul> <li>Artificial systems match human performance in object recognition tasks (Krizhevsky et al., 2012).</li> <li>Outperform expert humans in Atari video games (Mnih et al., 2015), Go (Silver et al., 2016), and heads-up poker (Morav\u010d\u00edk et al., 2017).</li> <li>Generate near-realistic synthetic images and human speech simulations (Lake et al., 2015; van den Oord et al., 2016).</li> <li>Enable multilingual translation (Wu et al., 2016) and create \"neural art\" mimicking famous painters (Gatys et al., 2015).</li> </ul> </li> <li> <p>Neuroscience Advancements:</p> <ul> <li>New tools in brain imaging and genetic bioengineering provide detailed insights into neural circuit computations (Deisseroth and Schnitzer, 2013).</li> <li>Promise a deeper understanding of mammalian brain function.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#future-intuitive-understanding-of-the-physical-world","title":"Future: Intuitive Understanding of the Physical World","text":"<ul> <li> <p>Core Human Cognitive Abilities:</p> <ul> <li>Human infants possess innate knowledge of physical world concepts like space, number, and objectness, enabling compositional mental models for inference and prediction (Gilmore et al., 2007; Gopnik and Schulz, 2004; Spelke and Kinzler, 2007; Battaglia et al., 2013).</li> </ul> </li> <li> <p>NN Architectures for Scene Reasoning:</p> <ul> <li>Novel architectures decompose scenes into objects and their relations, mimicking human-like reasoning (Battaglia et al., 2016; Chang et al., 2016; Eslami et al., 2016).</li> <li>Achieve human-level performance on complex reasoning tasks (Santoro et al., 2017).</li> </ul> </li> <li> <p>Deep RL for Commonsense:</p> <ul> <li>Deep RL models simulate how children develop commonsense through interactive experiments (Denil et al., 2016).</li> </ul> </li> <li> <p>Deep Generative Models for Object Representation:</p> <ul> <li>Models construct rich object models from raw sensory inputs, leveraging neuroscience-inspired constraints like redundancy reduction (Barlow, 1959; Higgins et al., 2016).</li> <li>Learned latent representations are disentangled (e.g., shape, position) and exhibit compositional properties, enabling flexible transfer to new tasks (Eslami et al., 2016; Higgins et al., 2016; Rezende et al., 2016a).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#future-efficient-learning","title":"Future: Efficient Learning","text":"<ul> <li> <p>Human Cognitive Efficiency:</p> <ul> <li>Humans rapidly learn new concepts from few examples, using prior knowledge for flexible inductive inferences.</li> <li>Excel in tasks like the \"characters challenge,\" distinguishing novel handwritten characters after one example (Lake et al., 2016).</li> </ul> </li> <li> <p>AI Progress in Few-Shot Learning:</p> <ul> <li>Structured probabilistic models enable inferences and sample generation from single examples (Lake et al., 2015).</li> <li>Deep generative models, based on DRAW, support one-shot concept learning and sample generation (Rezende et al., 2016b).</li> <li>Networks that \"learn to learn\" leverage prior experience for one-shot learning and faster RL task acquisition (Santoro et al., 2016; Vinyals et al., 2016; Wang et al., 2016).</li> </ul> </li> <li> <p>Neuroscience Roots:</p> <ul> <li>\"Learning to learn\" concept originates from animal learning studies (Harlow, 1949).</li> <li>Further explored in developmental psychology (Adolph, 2005; Kemp et al., 2010; Smith, 1995).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#future-transfer-learning","title":"Future: Transfer Learning","text":"<ul> <li> <p>AI Architectures for Generalization:</p> <ul> <li>Compositional representations enable zero-shot inferences for novel shapes outside training distributions (Higgins et al., 2016).</li> <li>Progressive networks transfer knowledge from one video game to another, supporting rapid learning and far transfer (Rusu et al., 2016a).</li> <li><code>Progressive networks</code> reduce training time by transferring knowledge from simulated robotic environments to real-world robot arms (Rusu et al., 2016b).</li> </ul> </li> <li> <p>Relation to Human Task Learning:</p> <ul> <li><code>Progressive network</code> architecture resembles computational models of sequential task learning in humans (Collins and Koechlin, 2012; Donoso et al., 2014).</li> <li>Deep networks solve visual analogies, indicating progress in relational reasoning (Reed et al., 2015).</li> </ul> </li> <li> <p>Neural Coding for Abstract Knowledge:</p> <ul> <li>Conceptual representations may encode invariant, relational information across sensory domains (Doumas et al., 2008).</li> <li>Grid codes in the entorhinal cortex, which encode allocentric space with hexagonal patterns, may support abstract reasoning (Constantinescu et al., 2016; Rowland et al., 2016).</li> <li>Functional neuroimaging suggests grid-like codes during abstract categorization tasks, indicating periodic encoding in human knowledge organization (Constantinescu et al., 2016).</li> <li>Grid codes may decompose state spaces efficiently, aiding subgoal discovery and hierarchical planning (Stachenfeld et al., 2014).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#future-imagination-and-planning","title":"Future: Imagination and Planning","text":"<ul> <li> <p>Deep RL Limitations:</p> <ul> <li>Deep RL (e.g., DQN) operates reactively, mapping perceptual inputs to actions to maximize future value (model-free RL).</li> <li>Drawbacks: data inefficiency (requires extensive experience) and inflexibility to outcome value changes (Daw et al., 2005).</li> </ul> </li> <li> <p>Human and Animal Planning:</p> <ul> <li>Humans use simulation-based planning, forecasting long-term outcomes via internal environmental models (Daw et al., 2005; Dolan and Dayan, 2013; Tolman, 1948).</li> <li>Animals (e.g., scrub jays, rats) exhibit planning: scrub jays consider future food recovery conditions (Raby et al., 2007); rats use cognitive maps for navigation and one-shot learning (Daw et al., 2005; Tolman, 1948).</li> </ul> </li> <li> <p>AI Planning Techniques:</p> <ul> <li>Early AI planning (e.g., Dyna) inspired by mental models for generating hypothetical experiences (Sutton, 1991; Craik, 1943).</li> <li>Model-based RL and Monte Carlo tree search (MCTS) enable forward search, contributing to expert-level Go performance (Silver et al., 2016; Browne et al., 2012).</li> </ul> </li> <li> <p>Hippocampal Role in Planning:</p> <ul> <li>Hippocampus supports simulation-based planning across species, creating spatially and temporally coherent imagined experiences (Hassabis and Maguire, 2007, 2009; Schacter et al., 2012).</li> <li>Rat hippocampal \"preplay\" activity during navigation resembles imagined trajectories (Johnson and Redish, 2007; \u00d3lafsd\u00f3ttir et al., 2015; Pfeiffer and Foster, 2013).</li> <li>Human non-spatial planning involves similar hippocampal processes (Doll et al., 2015; Kurth-Nelson et al., 2016).</li> </ul> </li> <li> <p>Deep Generative Models:</p> <ul> <li>Generate temporally consistent sequences reflecting realistic environments\u2019 geometric layouts (Eslami et al., 2016; Rezende et al., 2016a, 2016b; Gemici et al., 2017; Oh et al., 2015).</li> <li>Parallel hippocampal function in binding components for coherent imagined experiences (Hassabis and Maguire, 2007).</li> </ul> </li> <li> <p>Neuroscience Insights for AI:</p> <ul> <li>Hippocampus instantiates an internal environmental model; goal-contingent valuation occurs in orbitofrontal cortex or striatum (Redish, 2016).</li> <li>Prefrontal cortex may initiate hippocampal model roll-forward, paralleling AI proposals of bidirectional controller-model interactions (Schmidhuber, 2014).</li> <li>AI architectures separate controller and environmental model for simulation-based planning in physical object interactions (Hamrick et al., 2017).</li> </ul> </li> <li> <p>Human Imagination Characteristics:</p> <ul> <li>Constructive: humans recombine familiar elements into novel scenarios using compositional/disentangled representations (Eslami et al., 2016; Higgins et al., 2016; Rezende et al., 2016a).</li> <li>Hierarchical and \"jumpy\": planning spans multiple temporal scales, considering terminal goals, interim choices, and steps (Balaguer et al., 2016; Solway et al., 2014; Huys et al., 2012).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#future-virtual-brain-analytics","title":"Future: Virtual Brain Analytics","text":"<ul> <li> <p>Neuroscience Tools for AI Analysis:</p> <ul> <li>Neuroscience techniques (e.g., single-cell recording, neuroimaging, lesion studies) applied to AI systems enhance understanding of computations and representations in complex tasks.</li> <li>Termed \"virtual brain analytics\" for interpreting AI \"black boxes.\"</li> </ul> </li> <li> <p>Dimensionality Reduction:</p> <ul> <li>Common in neuroscience, dimensionality reduction visualizes brain states and has been adapted to analyze NN states (Zahavy et al., 2016).</li> </ul> </li> <li> <p>Receptive Field Mapping:</p> <ul> <li>Used to determine response properties of NN units.</li> <li>Activity maximization generates synthetic images by maximizing specific unit activity (Nguyen et al., 2016; Simonyan et al., 2013).</li> </ul> </li> <li> <p>Linearized Network Analysis:</p> <ul> <li>Neuroscience-inspired analyses of linearized networks reveal principles for optimizing learning, understanding network depth, and representational structure (McClelland and Rogers, 2003; Saxe et al., 2013).</li> </ul> </li> <li> <p>Advantages in AI Research:</p> <ul> <li>AI researchers have complete system knowledge and can causally manipulate components, enabling precise analysis and hypothesis-driven experiments (Jonas and Kording, 2017; Krakauer et al., 2017).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_1.html#from-ai-to-neuroscience","title":"From AI to Neuroscience","text":"<ul> <li> <p>Machine Learning in Neuroimaging:</p> <ul> <li>Machine learning enhances analysis of fMRI and MEG data through multivariate techniques (Cichy et al., 2014; \u00c7ukur et al., 2013; Kriegeskorte and Kievit, 2013).</li> <li>Promises advancements in connectomic analysis (Glasser et al., 2016).</li> </ul> </li> <li> <p>RL and Neuroscience:</p> <ul> <li>RL concepts, inspired by animal psychology, align with neural signals in midbrain dopaminergic neurons, resembling temporal difference (TD) prediction errors (O\u2019Doherty et al., 2003; Schultz et al., 1997).</li> <li>Suggests the brain implements TD learning.</li> </ul> </li> <li> <p>CNN and Visual Processing:</p> <ul> <li>CNN architectures explain neural representations in the ventral visual stream of humans and monkeys (Khaligh-Razavi and Kriegeskorte, 2014; Yamins and DiCarlo, 2016).</li> <li>Deep supervised networks account for increased coding of object properties (e.g., position, size) higher up the ventral visual stream (Hong et al., 2016).</li> </ul> </li> <li> <p>Long Short-Term Memory (LSTM) and Working Memory:</p> <ul> <li>LSTM properties inform models of working memory, supporting gating-based maintenance of task-relevant information in the prefrontal cortex (Lloyd et al., 2012; O\u2019Reilly and Frank, 2006).</li> </ul> </li> <li> <p>NN with External Memory:</p> <ul> <li>Memory-augmented networks enable iterative querying, critical for reasoning over multiple inputs (Sukhbaatar et al., 2015).</li> <li>Suggests potential hippocampal mechanisms for similar cognitive processes (Kumaran and McClelland, 2012).</li> </ul> </li> <li> <p>Meta-RL:</p> <ul> <li>RL optimizes recurrent network weights to create a faster-learning emergent RL algorithm (Duan et al., 2016; Wang et al., 2016).</li> <li>Connects to prefrontal cortex roles in RL alongside dopamine-based mechanisms (Tsutsui et al., 2016; Schultz et al., 1997).</li> </ul> </li> <li> <p>Backpropagation and Biological Plausibility:</p> <ul> <li>Random backward connections enable effective backpropagation without symmetric connectivity, relaxing biological implausibility (Liao et al., 2015; Lillicrap et al., 2016).</li> <li>Hierarchical auto-encoder and energy-based networks approximate backpropagation using local information, aligning with spike-timing dependent plasticity (Scellier and Bengio, 2016; Whittington and Bogacz, 2017).</li> <li>Local learning rules in supervised networks generate high-level invariances, such as mirror-symmetric tuning (Leibo et al., 2017).</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html","title":"2017 NeuroAI 2","text":""},{"location":"NeuroAI/2017_NeuroAI_2.html#deep-learning","title":"Deep Learning","text":"<ul> <li>1943, McCulloch and Pitts: NN that could compute logical functions</li> <li>1949, Hebb: efficiently encode environmental statistics in an unsupervised fashion</li> <li>1958, Rosenblatt: NN learn incrementally via supervisory feedback</li> <li>1980, Fukushima: early NN models of visual processing</li> <li>1985, Rumelhart: backprop</li> <li>2006, Hinton: deep belief networks</li> <li>2009, Deng: introduction of large datasets inspired by research on human language</li> <li>2012, Hinton: Dropout regularization, motivated by the stochasticity that is inherent in neurons that fire with Poisson-like statistics</li> <li>2015, LeCun: sentences can be represented as vectors</li> <li>2016, Yamins and DiCarlo: CNNs incorporate nonlinear transduction, divisive normalization, and maximum based pooling of inputs<ul> <li>1959, Hubel and Wiesel: single-cell recordings from the mammalian visual cortex revealed how visual input is filtered and pooled in simple and complex cells in area V1</li> <li>Replicates the hierarchical organization of mammalian cortical systems, with both convergent and divergent information flow in successive, nested processing layers </li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025","title":"SOTA 2025","text":"<ul> <li>LLMs</li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li> <p>TD methods </p> <ul> <li>Real-time models that learn from differences between temporally successive predictions, rather than having to wait until the actual reward is delivered. </li> <li>Of particular relevance was an effect called second-order conditioning, where affective significance is conferred on a conditioned stimulus (CS) through association with another CS rather than directly via association with the unconditioned stimulus. </li> <li>TD learning provides a natural explanation for second-order conditioning and indeed has gone on to explain a much wider range of findings from neuroscience.</li> </ul> </li> <li> <p>TD based RL: DQNs, A3C, PPO (TD for value estimation), SAC (TD for Q-function updates)  </p> <ul> <li>DQN uses TD learning by bootstrapping from its own predictions \\(Q_{\\text{target}}\\) to update Q-values in real time.</li> <li>Unlike Monte Carlo, it doesn\u2019t need to wait for final outcomes \u2014 it learns from temporal differences between successive predictions.</li> <li>The \"real-time\" aspect comes from the fact that every step generates a TD error, which is used to improve the policy immediately.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_1","title":"SOTA 2025","text":"<ul> <li>DreamerV3: Best for model-based RL from pixels</li> <li>MuZero: Combines planning + learning without knowing environment rules</li> <li>SAC: SOTA for continuous control</li> <li>PPO: Widely used, stable, scalable</li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#attention","title":"Attention","text":"<ul> <li>Traditionally, CNN models worked directly on entire images, with equal priority given to all pixels at the earliest stage of processing</li> <li>The primate visual system works differently.  Visual attention shifts strategically among locations and objects, centering processing resources and representational coordinates on a series of regions in turn</li> <li> <p>Attentional mechanisms have been a source of inspiration for AI architectures that take \"glimpses\" of the input image at each step, update internal state representations, and then select the next location to sample </p> <ul> <li>One such network was able to use this selective attentional mechanism to ignore irrelevant objects in a scene, allowing it to perform well in challenging object classification tasks in the presence of clutter </li> <li>2014. DeepMind. Recurrent Models of Visual Attention</li> </ul> </li> <li> <p>While attention is typically thought of as an orienting mechanism for perception, it can also be focused toward the contents of internal memory, this has helped provide recent successes in machine translation and memory + reasoning tasks</p> </li> <li> <p>One further area of AI where attention mechanisms have recently proven useful focuses on generative models that mimic the structure of examples presented during training</p> <ul> <li>For example, in one SOTA generative model known as DRAW, attention allows the system to build up an image incrementally, attending to one portion of a \"mental canvas\" at a time</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_2","title":"SOTA 2025","text":"<ul> <li>LLMs</li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#episodic-memory","title":"Episodic Memory","text":"<ul> <li> <p>Allow experiences to be encoded rapidly in a content-addressable store </p> <ul> <li>Associated with medial temporal lobe, (including hippocampus)</li> </ul> </li> <li> <p>Animal learning is supported by complementary learning systems in the hippocampus and neocortex</p> <ul> <li>The hippocampus acts to encode novel information after a single exposure (one-shot learning), but this information is gradually consolidated to the neocortex in sleep or resting periods that are interleaved with periods of activity. This consolidation is accompanied by replay in the hippocampus and neocortex, which is observed as a reinstatement of the structured patterns of neural activity that accompanied the learning event </li> <li>This theory was originally proposed as a solution to the well-known problem that in conventional neural networks, correlated exposure to sequential task settings leads to interference (catastrophic forgetting)</li> <li>The replay buffer in DQN is like a primitive hippocampus, permitting complementary learning in silico</li> <li>Enhanced when replay of highly rewarding events is prioritized (hippocampal replay seems to favor events that lead to high levels of reinforcement)<ul> <li>2015. DeepMind. Prioritized Experience Replay</li> </ul> </li> </ul> </li> <li> <p>DQN exhibits expert play on Atari video games by learning to transform image pixels to a policy</p> <ul> <li>Experience replay is critical to maximizing data efficiency, avoids the destabilizing effects of learning from consecutive correlated experiences, and allows the network to learn a viable value function even in complex sequential environments (video games)</li> </ul> </li> <li> <p>Episodic Control</p> <ul> <li>Experiences stored in a memory buffer can not only be used to gradually adjust the parameters of a deep network toward an optimal policy, as in DQN</li> <li>Can also support rapid behavioral change based on an individual experience. Neuroscience has argued for the potential benefits of episodic control, whereby rewarded action sequences can be internally re-enacted from a rapidly updatable memory store (hippocampus). Advantageous when limited experience has been obtained</li> <li>Recent AI research has drawn on these ideas to overcome the slow learning in deep RL<ul> <li>2016. DeepMind. Model-Free Episodic Control</li> </ul> </li> <li>These networks store experiences (e.g., actions and reward outcomes associated with particular game screens) and select new actions based on the similarity between the current input and memories, taking the reward associated with previous events into account</li> <li>Striking gains in performance over deep RL. Further, they are able to achieve success on tasks that depend heavily on one-shot learning, where typical deep RL architectures fail</li> <li>In the future, it will be interesting to harness the benefits of rapid episodic-like memory and more traditional incremental learning (Imagination and planning)</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_3","title":"SOTA 2025","text":"<ul> <li> <p>Problems with Episodic Control</p> <ul> <li>You can't store all the experience.</li> <li>Similar inputs don\u2019t always lead to similar outcomes.</li> </ul> </li> <li> <p>How World Models solves them</p> <ul> <li>Compression &amp; Generalization: World models summarize and compress many experiences into learned patterns</li> <li>Variance Reduction: world models learn structure and smooth out noise in the data</li> <li>Predictive Imagination: World models allow simulation of counterfactuals: What if I try a different action in this situation?</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#working-memory","title":"Working Memory","text":"<ul> <li> <p>Human working memory</p> <ul> <li>Thought to be instantiated within the prefrontal cortex and interconnected areas. </li> <li>Classic cognitive theories: depends on interactions between a central controller and separate, domain-specific memory buffers</li> </ul> </li> <li> <p>Began with RNN displaying attractor dynamics and rich sequential behavior, directly inspired by neuroscience</p> </li> <li> <p>One can see close parallels between the learning dynamics in these early, neuroscience-inspired networks and those in LSTM networks. LTSMs allow information to be gated into a fixed activity state and maintained until an appropriate output is required. The functions of sequence control and memory storage are closely intertwined instead of separate</p> </li> <li> <p>Differential neural computer (DNC) involves a neural network controller that attends to and reads/writes from an external memory matrix. </p> <ul> <li>This externalization allows the network controller to learn from scratch (i.e., via end-to-end optimization) to perform a wide range of complex memory and reasoning tasks that currently elude LSTMs, such as finding the shortest path through a graph-like structure</li> <li>These types of problems were previously argued to depend exclusively on symbol processing and variable binding and therefore beyond the purview of neural networks</li> </ul> </li> <li> <p>Although both LSTMs and the DNC are described here in the context of working memory, they have the potential to maintain information over many thousands of training cycles and so may thus be suited to longer-term forms of memory, such as retaining and understanding the contents of a book.</p> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_4","title":"SOTA 2025","text":"<ul> <li>Transformers (implicit memory)</li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#continual-learning","title":"Continual Learning","text":"<ul> <li> <p>Neuroscience</p> <ul> <li>Decreased synaptic lability (lower rates of plasticity) in a proportion of strengthened synapses, mediated by enlargements to dendritic spines that persist despite learning of other tasks. </li> <li>Theoretical models: memories can be protected from interference through synapses that transition between a cascade of states with different levels of plasticity.</li> </ul> </li> <li> <p>Elastic Weight Consolidation (EWC)</p> <ul> <li>2016. DeepMind. Overcoming catastrophic forgetting in neural networks</li> <li>Acts by slowing down learning in a subset of network weights identified as important to previous tasks. Allows deep RL networks to support continual learning at large scale.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#intuitive-understanding-of-the-physical-world","title":"Intuitive Understanding of the Physical World","text":"<ul> <li> <p>Novel neural network architectures </p> <ul> <li>Interpret and reason about scenes in a human-like way, by decomposing them into individual objects and their relations</li> <li>2016. DeepMind. Interaction Networks for Learning about Objects, Relations and Physics</li> <li>2016. DeepMind. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models</li> <li>2017. DeepMind. A simple neural network module for relational reasoning<ul> <li>Human-level performance on challenging reasoning tasks </li> </ul> </li> </ul> </li> <li> <p>Deep RL</p> <ul> <li>Capture the processes by which children gain commonsense understanding of the world through interactive experiments.</li> <li>2016. DeepMind. Learning to Perform Physics Experiments via Deep Reinforcement Learning </li> </ul> </li> <li> <p>Deep generative models </p> <ul> <li>Construct rich object models from raw sensory inputs </li> <li>2016. DeepMind. Early Visual Concept Learning with Unsupervised Deep Learning</li> <li>Leverage constraints first identified in neuroscience, such as redundancy reduction, which encourage the emergence of disentangled representations of independent factors such as shape and position.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_5","title":"SOTA 2025","text":"<ul> <li> <p>2021. UC Berkeley. Decision Transformer: Reinforcement Learning via Sequence Modeling</p> <ul> <li>An architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer</li> <li>Matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks</li> </ul> </li> <li> <p>2023. Meta. Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</p> <ul> <li>Grounded in the fact that humans learn an enormous amount of background knowledge about the world just by passively observing it.</li> <li>At a high level, the JEPA aims to predict the representation of part of an input (such as an image or piece of text) from the representation of other parts of the same input. </li> <li>Because it does not involve collapsing representations from multiple views/augmentations of an image to a single point, the hope is for the JEPA to avoid the biases and issues associated with another widely used method called invariance-based pretraining.</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#efficient-learning","title":"Efficient Learning","text":"<ul> <li>Learn to learn <ul> <li>Acquiring knowledge on new tasks by leveraging prior experience with related problems, to support one-shot concept learning and accelerating learning in RL tasks</li> <li>2016. DeepMind. One-shot Learning with Memory-Augmented Neural Networks (MANN)<ul> <li>The Neural Turing Machine (NTM) is a fully differentiable implementation of a MANN</li> </ul> </li> <li>2016. DeepMind. Matching Networks for One Shot Learning<ul> <li>A new neural architecture that, by way of its corresponding training regime, is capable of state-of-the-art performance on a variety of one-shot classification tasks.</li> </ul> </li> <li>2016. DeepMind. Learning to reinforcement learn</li> <li>2017. Finn. Model-Agnostic Meta-Learning (MAML) for Fast Adaptation of Deep Networks</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#transfer-learning","title":"Transfer Learning","text":"<ul> <li> <p>Progressive Neural Networks: a new class of architecture</p> <ul> <li>2016. DeepMind. Progressive Neural Networks</li> <li>2016. DeepMind. Sim-to-Real Robot Learning from Pixels with Progressive Nets</li> <li>Leverage knowledge gained in one video game to learn rapidly in another</li> <li>The proposed architecture bears some resemblance to a successful computational model of sequential task learning in humans.</li> </ul> </li> <li> <p>Neuroscience</p> <ul> <li>How humans or other animals achieve this sort of high-level transfer learning is unknown, and remains a relatively unexplored topic in neuroscience</li> <li>At the level of neural coding, this kind of transfer of abstract structured knowledge may rely on the formation of conceptual representations that are invariant to the objects, individuals, or scene elements that populate a sensory domain but code instead for abstract, relational information among patterns of inputs (lack direct evidence)</li> <li>One recent report: neural codes thought to be important in the representation of allocentric (map-like) spaces might be critical for abstract reasoning in more general domains</li> <li>In the mammalian entorhinal cortex, cells encode the geometry of allocentric space with a periodic \"grid\" code, with receptive fields that tile the local space in a hexagonal pattern (Rowland et al., 2016)</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_6","title":"SOTA 2025","text":"<ul> <li> <p>LLMs!</p> </li> <li> <p>World Models</p> <ul> <li>I-JEPA: The first AI model based on Yann LeCun's vision for more human-like AI</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#imagination-and-planning","title":"Imagination and Planning","text":"<ul> <li> <p>Model-free RL</p> <ul> <li>Computationally inexpensive</li> <li>Data inefficient</li> <li>Inflexible (insensitive to changes in the value of outcomes)</li> <li>2005. Daw. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</li> <li>2013. Dolan and Dayan. Goals and Habits in the Brain</li> <li>1948. Tolman. Cognitive maps in rats and men</li> </ul> </li> <li> <p>Monte Carlo tree search (MCTS): use forward search to update a value function and/or policy</p> <ul> <li>2012. Browne. A survey of Monte Carlo tree search methods</li> <li>2016. Silver. Mastering the game of Go with deep neural networks and tree search</li> </ul> </li> <li> <p>Limitations</p> <ul> <li>How rich internal models can be learned through experience without strong priors being handcrafted into the network by the experimenter?</li> </ul> </li> <li> <p>Hippocampus: imagine possible scenarios &amp; simulation based planning</p> <ul> <li>Example: when paused at a choice point, ripples of neural activity in the rat hippocampus resemble those observed during subsequent navigation of the available trajectories (\"preplay\"), as if the animal were \"imagining\" each possible alternative</li> <li>Supports planning by instantiating an internal model of the environment, with goal-contingent valuation of simulated outcomes occurring in areas downstream of the hippocampus such the orbitofrontal cortex or striatum</li> <li>Mechanisms that guide the rolling forward of an internal model of the environment in the hippocampus remain uncertain and merit future scrutiny (One possibility is that this process is initiated by the prefrontal cortex through interactions with the hippocampus)</li> <li>2017. Hamrick. Metacontrol for Adaptive Imagination-Based Optimization</li> </ul> </li> <li> <p>Deep generative models for simulation-based planning</p> <ul> <li>2016. Eslami. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models</li> <li>2016. Rezende. Unsupervised Learning of 3D Structure from Images</li> <li>2016. Rezende. One-Shot Generalization in Deep Generative Models</li> <li>Generate temporally consistent sequences of generated samples that reflect the geometric layout of newly experienced realistic environments<ul> <li>2017. Gemici. Generative Temporal Models with Memory</li> <li>2015. Oh. Action-Conditional Video Prediction using Deep Networks in Atari Games</li> </ul> </li> <li>Using these models for simulation-based planning in agents remains a challenge for future work</li> </ul> </li> <li> <p>Human imagination</p> <ul> <li>Construct fictitious mental scenarios by recombining familiar elements in novel ways</li> <li>Involves efficient representations that support generalization and transfer</li> <li>\"Jumpy\": bridging multiple temporal scales at a time</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_7","title":"SOTA 2025","text":"<ul> <li> <p>Model-Based RL &amp; World Models</p> <ul> <li>2025. DeepMind. Improving Transformer World Models for Data-Efficient RL</li> <li>2023. DeepMind. Mastering Diverse Domains through World Models</li> <li>2024. DeepMind. Genie 2: A large-scale foundation world model</li> </ul> </li> <li> <p>2023. Wang. Voyager: An Open-Ended Embodied Agent with Large Language Models</p> </li> <li> <p>2024. DeepMind. AlphaGeometry: An Olympiad-level AI system for geometry</p> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#virtual-brain-analytics","title":"Virtual Brain Analytics","text":"<ul> <li> <p>Equivalents of single-cell recording, neuroimaging, and lesion techniques</p> </li> <li> <p>Neuroscience: visualizing brain states through dimensionality reduction</p> <ul> <li>2016. Zahavy. Graying the black box: Understanding DQNs</li> </ul> </li> <li> <p>Neuroscience: Receptive field mapping</p> <ul> <li>2013. Simonyan. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</li> <li>Activity maximization: a network learns to generate synthetic images by maximizing the activity of certain classes of unit<ul> <li>2016. Nguyen. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</li> </ul> </li> </ul> </li> <li> <p>Neuroscience-inspired analyses of linearized networks</p> <ul> <li>2003. McClelland. The parallel distributed processing approach to semantic cognition</li> <li>2013. Saxe. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</li> </ul> </li> <li> <p>Networks with external memory</p> <ul> <li>2016. Graves. Hybrid computing using a neural network with dynamic external memory</li> </ul> </li> <li> <p>Hypothesis-driven experiments</p> <ul> <li>2017. Jonas and Kording. Could a Neuroscientist Understand a Microprocessor?</li> <li>2017. Krakauer. Neuroscience Needs Behavior: Correcting a Reductionist Bias</li> </ul> </li> <li> <p>Circuits</p> <ul> <li>2020. Olah. Zoom In: An Introduction to Circuits</li> </ul> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#sota-2025_8","title":"SOTA 2025","text":"<ul> <li> <p>Netron: model visualizer</p> </li> <li> <p>pytorch/captum: Model interpretability and understanding</p> </li> <li> <p>BertViz: Visualize Attention</p> </li> </ul>"},{"location":"NeuroAI/2017_NeuroAI_2.html#extra","title":"Extra","text":"<ul> <li>google/neural-tangents: Infinite Width Neural Networks</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html","title":"Neuromatch course","text":"<ul> <li>Notes for NeuroAI course by Neuromatch Academy</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#111-generalization-in-ai","title":"1.1.1: Generalization in AI","text":"<ul> <li>TrOCR: Transformer-based OCR model</li> <li>some strategies for better generalization: Transfer learning, Augmentations, Synthetic examples</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#112-generalization-in-neuroscience","title":"1.1.2: Generalization in Neuroscience","text":"<ul> <li>use RNN to mimic the brain and generate muscle control sequence<ul> <li>weight regularization (L2) and firing rate regularization for generalization</li> </ul> </li> </ul> \\[ \\tau { dh \\over dt } = -h + W_1 x + W_2 |\\tanh(h)| + b \\] <ul> <li>\\(h\\): hidden state, \\(x\\): input</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#113-generalization-in-cognitive-science","title":"1.1.3: Generalization in Cognitive Science","text":"<ul> <li> <p>2020, Learning Task-General Representations with Generative Neuro-Symbolic Modeling</p> <ul> <li>Humans display one-shot learning on Omniglot, a character recognition task. This requires extensive generalization.</li> <li>A generative neuro-symbolic model with strong inductive biases exhibits human-level performance on Omniglot.</li> </ul> </li> <li> <p>Sample complexity: minimum number of examples needed to reach a specific performance with some probability</p> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#121-task-definition-application-relations-and-impacts-on-generalization","title":"1.2.1: Task definition, application, relations and impacts on generalization","text":""},{"location":"NeuroAI/Neuromatch_course.html#classification-cross-entropy-loss","title":"classification: cross-entropy loss","text":"\\[ L = - {1\\over N} \\sum_i^N \\log( { \\exp(z_{i, k}) \\over \\sum_c \\exp( z_{i, c} ) } ) \\\\ z_{i, c}: \\text{ logits } \\\\ k: \\text{ true class for sample } i \\]"},{"location":"NeuroAI/Neuromatch_course.html#regression-mse-loss","title":"regression: MSE loss","text":"\\[ L = {1\\over N} \\sum_i^N ( y_i - \\hat{y}_i )^2 \\]"},{"location":"NeuroAI/Neuromatch_course.html#auto-encoding-mse-loss","title":"auto-encoding: MSE loss","text":"<ul> <li>MSE loss between the original and reconstructed inputs</li> <li>auto-encoder consists of: an encoder, a bottleneck layer, and a decoder</li> <li>compresses the input into a smaller representation, the bottleneck layer holds this compressed representation, and the decoder reconstructs the original image from this representation</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#inpainting-mse-loss","title":"inpainting: MSE loss","text":"<ul> <li>an auto-encoder that can fill in missing parts of an image</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#122-contrastive-learning-for-object-recognition","title":"1.2.2: Contrastive learning for object recognition","text":"<ul> <li> <p>contrastive learning (self-supervised)</p> <ul> <li>works well in situations where the number of classes is large or undefined</li> <li>learns to distinguish between 'similar' and 'dissimilar' directly through embeddings</li> <li>example: identify faces among billions of possibilities</li> </ul> </li> <li> <p>Residual networks: </p> <ul> <li>easier to optimize: allows for a passageway for gradients to flow down during backprop</li> </ul> </li> <li> <p>Noise-Contrastive Estimation with Information (InfoNCE) loss function</p> </li> </ul> \\[ L = - E \\left[     \\log { f(x^+) \\over f(x^+) + \\sum_{x^-} f(x^-) } \\right] \\\\[5pt] f(x') := \\exp( s(x, x') / \\tau ) \\] <ul> <li>$ x $: anchor data point (an input sample)</li> <li>$ x^+ $: transformed / augmented version of $ x $ (similar)</li> <li>$ x^- $: randomly sampled from dataset (dissimilar)</li> <li>$ s(x, x') $: some similarity function between embeddings of $ x $ and $ x' $</li> <li>$ \\tau $: temperature</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#123-reinforcement-learning-across-temporal-scales","title":"1.2.3: Reinforcement learning across temporal scales","text":"<ul> <li>refer to 2018 Prefrontal cortex meta-RL</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#131-generalization-and-representational-geometry","title":"1.3.1: Generalization and representational geometry","text":"<ul> <li> <p>Adversarial data: attacker-designed data that cause models to make mistakes, but indistinguishable by humans</p> <ul> <li>Fast Gradient Sign Method (FGSM, Ian Goodfellow, 2014): backprops through the NN to create perturbed inputs that maximize the loss. $ x' = x + \\epsilon \\cdot \\text{sign}(\\nabla_x L) $</li> </ul> </li> <li> <p>Representational Dissimilarity Matrices (RDM) </p> <ul> <li>$ M_{ij} = 1 - r(h_i, h_j)$, $ r $ is Pearson correlation</li> <li>measures how dissimilar the response patterns in some layer of a NN are to the $ i $-th and $ j $-th input</li> <li>measure of generalization</li> </ul> </li> </ul> <p></p>"},{"location":"NeuroAI/Neuromatch_course.html#132-computation-as-transformation-of-representational-geometries","title":"1.3.2: Computation as transformation of representational geometries","text":"<ul> <li>Dimensionality reduction<ul> <li>PCA (Principal Component Analysis)</li> <li>MDS (Multi-Dimensional Scaling)</li> <li>t-SNE (t-distributed Stochastic Neighbor Embedding)</li> </ul> </li> </ul> <ul> <li>Representational path: from comparing representations to comparing RDMs<ol> <li>calculate RDMs based on the Euclidean distances between the representations of inputs (for each layer): $ M(\\text{layer}_k, x_i, x_j) $</li> <li>reshape: \\(K, I, J \\rightarrow K, I\\times J\\), then calculate the cosine similarity between different rows of this new matrix. By taking the <code>arccos</code> of this measure, we obtain a proper distance between representational geometries of different layers. an RDM matrix of the RDM matrices!</li> <li>visualize the path: embed the distances between the geometries in a lower dimensional space. use <code>MDS</code> to reduce the dimensions to 2</li> </ol> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#133-statistical-inference-on-representational-geometries","title":"1.3.3: Statistical inference on representational geometries","text":"<ul> <li> <p>Representational Similarity Analysis (RSA)</p> <ul> <li>Uses rsatoolbox to compute <code>Representational Dissimilarity Matrices (RDM)</code>, which capture pairwise dissimilarities in neural (fMRI) or model (AlexNet) responses to stimuli</li> <li>Compares fMRI RDMs with AlexNet RDMs by calculating the <code>correlation coefficient</code></li> </ul> </li> <li> <p>Bootstrapping (resampling)</p> </li> </ul> <p></p> <ul> <li>Student's t-test</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#134-representational-geometry-noise","title":"1.3.4: Representational geometry &amp; noise","text":"<ul> <li> <p>Goal: to develop more accurate and robust classification models</p> </li> <li> <p>Euclidean vs Mahalanobis distance</p> <ul> <li>Mahalanobis takes into account the noise covariances between neurons</li> <li>Euclidean assumes isotropic noise</li> </ul> </li> </ul> \\[ d_E^2 = (x - y)(x - y)^T \\\\ d_M^2 = (x - y) \\Sigma^{-1} (x - y)^T \\\\ \\Sigma \\text{: covariance matrix of the data} \\] <ul> <li>discriminability between stimulus pairs<ul> <li>If we assume noise is <code>i.i.d</code> across neurons (<code>isotropic</code>) and i.i.d across stimuli (homoscedastic), then Euclidean distance (in response space) defines discriminability between a pair of stimuli</li> <li>If we assume noise is <code>correlated</code> across neurons (<code>non-isotropic</code>) and i.i.d across stimuli (homoscedastic), then Mahalanobis defines discriminability</li> </ul> </li> <li> <p>Linear Discriminant Analysis</p> <ul> <li>For example, in a two-class problem, LDA finds a line (or hyperplane in higher dimensions) that best distinguishes the classes</li> </ul> </li> <li> <p>Cross-validated (train-test split) distance estimators can remove the positive bias introduced by noise</p> </li> </ul> \\[ d_E^2 = (x - y)_{\\text{train}}(x - y)_{\\text{test}}^T \\\\ d_M^2 = (x - y)_{\\text{train}} \\Sigma_{\\text{train}}^{-1} (x - y)_{\\text{test}}^T \\] <ul> <li>Johnson\u2013Lindenstrauss lemma: <ul> <li>states that a set of points in a high-dimensional space can be projected into a lower-dimensional space using a random linear projection (e.g., a random matrix) while approximately preserving the Euclidean distances between the points.</li> <li>shows that random projections preserve the Euclidean distance with some distortions. Crucially, the distortion does not depend on the dimensionality of the original space</li> <li>crucial for reducing dimensionality while preserving the relational structure of data</li> </ul> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#151-sparsity-and-sparse-coding-regularization","title":"1.5.1: Sparsity and Sparse Coding (regularization)","text":"<ul> <li>kurtosis: one of the main metrics used to measure sparsity</li> </ul> <pre><code>from sklearn.decomposition import DictionaryLearning, PCA\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\n</code></pre> <ul> <li> <p>Dictionary Learning: decompose complex data into simpler components, akin to how the visual system breaks down images into edges and textures etc.</p> </li> <li> <p>Orthogonal Matching Pursuit (OMP) algorithm: achieves \"sparse coding\" by deriving fundamental units (basis) that constitute complex signals</p> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#152-normalization","title":"1.5.2: Normalization","text":"<ul> <li>by adding a normalization layer<ul> <li>the training process converges quicker</li> <li>achieve better test accuracy and generalization in image recognition</li> </ul> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#153-attention","title":"1.5.3: Attention","text":""},{"location":"NeuroAI/Neuromatch_course.html#211-depth-vs-width","title":"2.1.1: Depth vs width","text":""},{"location":"NeuroAI/Neuromatch_course.html#212-double-descent-grokking","title":"2.1.2!!!: Double descent (Grokking)","text":"<ul> <li> <p>Double Descent:</p> <ul> <li>the situation when the over-parameterized network was expected to behave as over-fitted but instead generalized better to the unseen data. </li> <li>Moreover, we discovered how noise, regularization &amp; initial scale impact the effect of double descent and, in some cases, can fully cancel it.</li> <li>Combining with the findings on sparse connections and activity, brain architecture suggests it might operate in over parameterized regime, which enables the interpolation of vast amounts of sensory and experiential data smoothly, leading to effective generalization.</li> </ul> </li> <li> <p>Grokking</p> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#2022-grokking","title":"<code>2022</code> Grokking","text":"<ul> <li>OpenAI</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#213-neural-network-modularity","title":"2.1.3: Neural network modularity","text":"<ul> <li>modular architecture with separate modules for learning different aspects of behavior, is superior to a holistic architecture with a single module. The modular architecture with stronger inductive bias achieves good performance faster and has the capability to generalize to other tasks as well. this modularity is a property we also observe in the brains.</li> <li>env: spatial navigation task</li> <li>RL: actor-critic</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#221-basic-operations-of-vector-symbolic-algebra","title":"2.2.1: Basic operations of vector symbolic algebra","text":"<ul> <li>these 3 tutorials depends on sspspace (Spatial Semantic Pointers)<ul> <li>6 stars in 2 years</li> </ul> </li> <li>it is related to nengo<ul> <li>863 stars in 6 years</li> </ul> </li> <li>not mainstream AI, also doesn't seem to make much sense / be useful. will be skipped.</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#222-learning-with-structure","title":"2.2.2: Learning with structure","text":""},{"location":"NeuroAI/Neuromatch_course.html#223-representations-in-continuous-space","title":"2.2.3: Representations in continuous space","text":""},{"location":"NeuroAI/Neuromatch_course.html#231-microlearning-local","title":"2.3.1!!!: Microlearning (local)","text":"<ul> <li>Weight Perturbation</li> </ul> \\[ W' = W + Z \\quad Z \\sim N(0, \\sigma^2) \\\\ \\Delta W = -\\eta {dL \\over dW} \\\\[5pt] = -\\eta E_Z \\left[     ({dL \\over dW}^T Z) { Z \\over \\sigma^2 } \\right] \\\\[5pt] \\approx -\\eta E_Z \\left[     (L(Z) - L(0)) { Z \\over \\sigma^2 } \\right] \\] <ul> <li>Node Perturbation</li> </ul> \\[ r' = r + z \\quad z \\sim N(0, \\sigma^2) \\\\ \\Delta W = -\\eta E_z \\left[     (L(z) - L(0)) { z \\over \\sigma^2 } x^T \\right] \\] <ul> <li>Feedback Alignment</li> <li>Kolen-Pollack</li> </ul> <pre><code>from typing import List\n\nimport matplotlib.pyplot as plt\nimport torch as tc\nimport torch.nn as nn\nfrom torch.optim import Adam, Optimizer\n\n\nclass WeightPerturb(Optimizer):\n    def __init__(s, params, sigma=1e-2, lr=1e-3, K=10):\n        s.sig, s.lr, s.K = sigma, lr, K\n        super(WeightPerturb, s).__init__(params, defaults={})\n\n    def step(s, get_loss):\n        l0 = get_loss()\n        ps: List[tc.Tensor] = [p for g in s.param_groups for p in g[\"params\"]]\n        p0s = [p.data.clone() for p in ps]\n        gs = [tc.zeros(p.shape) for p in ps]\n        for _ in range(s.K):\n            zs = [tc.normal(0, s.sig, p.shape) for p in ps]\n            for p, p0, z in zip(ps, p0s, zs):\n                p.data = p0 + z\n            lz = get_loss()\n            for i, z in enumerate(zs):\n                gs[i] += (lz - l0) * z / s.sig**2\n        for p, p0, g in zip(ps, p0s, gs):\n            p.data = p0 - s.lr * g / s.K\n        return l0\n\n\ndef run_opt(Opt, lr=1e-3):\n    tc.manual_seed(1)\n    net = nn.Linear(6, 1)\n    x = tc.rand((100, 6))\n    y = tc.mean(x, dim=1, keepdim=True)\n    loss_fn = nn.MSELoss()\n    opt = Opt(net.parameters(), lr=lr)\n    losses = []\n\n    def get_loss() -&gt; tc.Tensor:\n        return loss_fn(y, net(x))\n\n    for i in range(1000):\n        if isinstance(opt, Adam):\n            opt.zero_grad()\n            loss = get_loss()\n            loss.backward()\n            opt.step()\n        elif isinstance(opt, WeightPerturb):\n            loss = opt.step(get_loss)\n        losses.append(loss.item())\n    plt.plot(losses, label=opt.__class__.__name__)\n\n\nif __name__ == \"__main__\":\n    run_opt(Adam)\n    run_opt(WeightPerturb)\n    plt.legend()\n    plt.savefig(\"test\")\n</code></pre>"},{"location":"NeuroAI/Neuromatch_course.html#241-the-problem-of-changing-data-distributions","title":"2.4.1: The problem of changing data distributions","text":"<ul> <li>Covariate shift: $ x $ changes.  </li> <li>Concept shift: $ f $ changes.</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#242-continual-learning","title":"2.4.2: Continual learning","text":"<p>solves catastrophic forgetting</p>"},{"location":"NeuroAI/Neuromatch_course.html#243-meta-learning","title":"2.4.3: Meta-learning","text":""},{"location":"NeuroAI/Neuromatch_course.html#2017-model-agnostic-meta-learning-maml","title":"<code>2017</code> Model-Agnostic Meta-Learning (MAML)","text":"<ul> <li> <p>Sergey Levine, Pieter Abbeel</p> </li> <li> <p>ABSTRACT</p> <ul> <li>In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient RL.</li> </ul> </li> </ul> <p> </p>"},{"location":"NeuroAI/Neuromatch_course.html#244-biological-meta-reinforcement-learning","title":"2.4.4: Biological meta reinforcement learning","text":""},{"location":"NeuroAI/Neuromatch_course.html#advantage-actor-critic-a2c","title":"Advantage Actor Critic (A2C)","text":"<ul> <li>Policy (Actor): selects actions using a policy $ \\pi_\\theta(a|s) $. Value Function (Critic): estimates the state value $ V_\\phi(s) $. Empirical Return (Monte Carlo). Advantage</li> </ul> \\[ \\pi_\\theta(a|s) = \\text{softmax}(W_{\\text{actor}} \\cdot h + b_{\\text{actor}}) \\\\ V_\\phi(s) = W_{\\text{critic}} \\cdot h + b_{\\text{critic}} \\\\ Q(s_t, a_t) = \\sum_{k=0}^{T-t} \\gamma^k r_{t+k} \\\\ A(s_t, a_t) = Q(s_t, a_t) - V_\\phi(s_t) \\] <ul> <li>Policy Gradient Loss, Value Function Loss, Entropy Regularization</li> </ul> \\[ \\mathcal{L}_{\\text{actor}} = -\\mathbb{E} \\left[ \\log \\pi_\\theta(a|s) \\cdot A(s,a) \\right] \\\\ \\mathcal{L}_{\\text{critic}} = \\mathbb{E} \\left[ (Q(s,a) - V_\\phi(s))^2 \\right] \\\\ \\mathcal{L}_{\\text{entropy}} = -\\mathbb{E} \\left[ \\sum_a \\pi_\\theta(a|s) \\log \\pi_\\theta(a|s) \\right] \\\\ \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{actor}} + \\beta_v \\mathcal{L}_{\\text{critic}} - \\beta_e \\mathcal{L}_{\\text{entropy}} \\]"},{"location":"NeuroAI/Neuromatch_course.html#baldwin-effect-meta-learning","title":"Baldwin effect: meta-learning","text":"<ul> <li> <p>Baldwin effect</p> <ul> <li>learned behaviors indirectly shape genetic evolution through natural selection (favoring traits that facilitate learning), rather than implying that acquired skills are directly encoded into genes (Lamarckian)</li> <li>we don't inherit the features / weights that make us good at specific tasks but rather the ability to learn quickly to gain the needed features</li> <li>evolution will select organisms that are good at learning</li> </ul> </li> <li> <p>implementation</p> <ul> <li>Outer Loop: Genetic Algorithm (finds good initial parameters, those good at learning)<ul> <li>Inner Loop: Standard RL</li> </ul> </li> </ul> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#2018-prefrontal-cortex-meta-rl","title":"<code>2018</code> Prefrontal cortex meta-RL","text":"<ul> <li> <p>Seems this paper lacks detailed mechanistic algorithms / code / math, need to investigate further</p> </li> <li> <p>Demis Hassabis</p> </li> <li> <p>ABSTRACT</p> <ul> <li>Over the past 20 years, neuroscience research on reward-based learning has converged on a canonical model: dopamine 'stamps in' associations between situations, actions and rewards by modulating synaptic strength. However, a growing number of recent findings have placed this standard model under strain. </li> <li>We introduce a new theory where the dopamine system trains the prefrontal cortex, to operate as its own free-standing learning system. </li> </ul> </li> </ul> <p></p> <ol> <li>Architecture: The prefrontal network (PFN), including sectors of the basal ganglia and the thalamus that connect directly with PFC, is modeled as a RNN, with synaptic weights adjusted through an RL algorithm driven by dopamine (DA). o = perceptual input. a = action. r = reward. v = state value. \u03b4 = reward prediction error (RPE)</li> <li>Learning: As suggested in past research, we assume that the synaptic weights within the prefrontal network, are adjusted by a model-free RL procedure, within which DA conveys a RPE signal. Via this role, the DA-based RL procedure shapes the activation dynamics of the recurrent prefrontal network.</li> <li> <p>Task environment: Following past proposals, we assume that RL takes place not on a single task, but instead in a dynamic environment posing a series of interrelated tasks. The learning system is thus required to engage in ongoing inference and behavioral adjustment. </p> </li> <li> <p>these 3 premises are all firmly grounded in existing research. The novel contribution here is to identify an emergent effect that results when the 3 premises are concurrently satisfied. As we will show, these conditions, when they co-occur, are sufficient to produce a form of meta-learning, where one learning algorithm gives rise to a second, more efficient learning algorithm. Specifically, by adjusting the connection weights within the prefrontal network, DA-based RL creates a second RL algorithm, implemented entirely in the prefrontal network's activation dynamics. This new learning algorithm is independent of the original one, and differs in ways that are suited to the task environment. Crucially, the emergent algorithm is a full-fledged RL procedure: It copes with the exploration-exploitation tradeoff, maintains a representation of the value function, and progressively adjusts the action policy. In view of this point, and in recognition of some precursor research, we refer to the overall effect as meta-reinforcement learning. </p> </li> </ol>"},{"location":"NeuroAI/Neuromatch_course.html#245-replay-buffer","title":"2.4.5: Replay Buffer","text":""},{"location":"NeuroAI/Neuromatch_course.html#251-consciousness","title":"2.5.1: Consciousness","text":"<ul> <li>Objectives<ul> <li>the hard problem of consciousness</li> <li>phenomenal consciousness vs access consciousness</li> <li>consciousness vs sentience vs intelligence</li> <li>reductionist theories: Global Workspace Theory (GWT), metacognition, Higher-Order Thought (HOT)</li> </ul> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#2020-recurrent-independent-mechanisms-rim-modularity","title":"<code>2020</code> Recurrent Independent Mechanisms (RIM): <code>modularity</code>","text":"<ul> <li> <p>Sergey Levine, Yoshua Bengio, Bernhard Sch\u00f6lkopf</p> </li> <li> <p>ABSTRACT</p> <ul> <li>a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate sparingly through the bottleneck of attention, and compete with each other so they are updated only at time steps where they are most relevant </li> <li>leads to specialization amongst the RIMs, remarkably improved generalization on tasks where some factors of variation differ systematically between training and evaluation</li> </ul> </li> <li> <p>RIM vs RNN (LSTM, GRU)</p> <ul> <li>The RIM cells are sparsely activated, only a subset of them are active at each time step</li> <li>cells are mostly independent, do not share weights or hidden states</li> <li>communicate with each other through an attention mechanism</li> </ul> </li> </ul> <p></p> <ul> <li>A single step has 4 stages <ol> <li>individual RIMs produce a query which is used to read from the current input</li> <li>attention is used to select which RIMs to activate </li> <li>activated RIMs follow their own transition dynamics</li> <li>RIMs sparsely communicate between themselves using attention</li> </ol> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#2022-global-workspace-theory-gwt-coordination","title":"<code>2022</code> Global Workspace Theory (GWT): <code>coordination</code>","text":"<ul> <li> <p>Yoshua Bengio</p> </li> <li> <p>ABSTRACT</p> <ul> <li>Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state:<ul> <li>Transformers segment by position</li> <li>object-centric architectures decompose images into entities </li> </ul> </li> <li>In all these architectures, interactions between different elements are modeled via pairwise interactions: <ul> <li>Transformers make use of self-attention to incorporate information from other positions </li> <li>object-centric architectures make use of graph neural networks to model interactions among entities </li> </ul> </li> <li>We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks </li> <li>In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. </li> <li>The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that:<ul> <li>they encourage specialization and compositionality</li> <li>they facilitate the synchronization of otherwise independent specialists</li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>workflow</p> <ol> <li>an ensemble of specialist modules doing their own default processing; at a particular computational stage, depending upon the input, a subset of the specialists becomes active.</li> <li>the active specialists get to write information in a shared global workspace.</li> <li>the contents of the workspace are broadcast to all specialists.</li> </ol> </li> <li> <p><code>1988</code> GWT of consciousness: consciousness arises from the ability of various brain processes to access a shared information platform, the Global Workspace.</p> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#second-order-model","title":"Second order model","text":"<ul> <li> <p>Blindsight: people with damaged primary visual cortex can still respond to visual stimuli without consciously perceiving them</p> </li> <li> <p>This section defines two neural networks working together:  </p> <ol> <li>FirstOrderNetwork: Processes input data (100D \u2192 hidden layer \u2192 100D output).  </li> <li>SecondOrderNetwork: Compares the input and output of the first network, then outputs a \"confidence score\" (called a wager) indicating how well the first network performed.  </li> </ol> </li> <li>Main Idea: The system not only makes predictions but also self-evaluates their reliability. The second network acts like a \"critic\" assessing the first network's performance on each input.</li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#higher-order-state-space-hoss-model","title":"Higher Order State Space (HOSS) model","text":"<ul> <li> <p>higher-order theory: consciousness stems from the ability to monitor basic, or first-order, information processing activities, instead of merely broadcasting information globally</p> </li> <li> <p>global ignition responses: big surges in brain activity that happen when we become conscious of something</p> </li> </ul>"},{"location":"NeuroAI/Neuromatch_course.html#252-ethics","title":"2.5.2: Ethics","text":""},{"location":"trading/options.html","title":"Options","text":""},{"location":"trading/options.html#black-scholes","title":"Black-Scholes \u6a21\u578b: \u4e3a\u4e86\u56de\u7b54 \"\u671f\u6743\u5e94\u5982\u4f55\u516c\u5e73\u5b9a\u4ef7\" \u8fd9\u4e2a\u95ee\u9898","text":""},{"location":"trading/options.html#st","title":"\u5047\u8bbe\u80a1\u7968\u4ef7\u683c $ S(t) $ \u7684\u52a8\u529b\u5b66\u4e3a:","text":"\\[  dS = S ( r \\; dt + \\sigma \\; dW_t ) \\\\[5pt] \\Longrightarrow S(t) = S_0 \\exp[ (r - \\sigma^2/2) \\; t + \\sigma W_t ] \\quad \\text{Ito's calculus} \\] <p>\u5176\u4e2d $ r $: \u6f02\u79fb\u7387, $ \\sigma $: \u6ce2\u52a8\u7387, $ dW_t \\sim N(\\mu=0, \\sigma^2=dt) $: \u7ef4\u7eb3\u8fc7\u7a0b (\u6807\u51c6\u5e03\u6717\u8fd0\u52a8)</p>"},{"location":"trading/options.html#vs-t","title":"\u5047\u8bbe\u671f\u6743\u4ef7\u683c $ V(S, t) $, \u6784\u5efa\u5bf9\u51b2\u7ec4\u5408","text":"<ul> <li>\u6839\u636e Ito's lemma (\u76f8\u5f53\u4e8e\u5e26\u566a\u58f0\u7684\u6cf0\u52d2\u5c55\u5f00)</li> </ul> \\[ dV = { \\partial V \\over \\partial t } dt + { \\partial V \\over \\partial S } dS + {1\\over 2} {\\partial^2 V \\over \\partial S^2} (dS)^2 \\\\[5pt] (dS)^2 = \\sigma^2 S^2 dt + O(dt^2) \\\\[5pt] \\Longrightarrow dV = { \\partial V \\over \\partial t } dt         + { \\partial V \\over \\partial S } dS         + {1\\over 2} \\sigma^2 S^2 { \\partial^2 V \\over \\partial S^2 } dt \\] <ul> <li>\u4e3a\u4e86\u6d88\u9664 \\(dS\\) (\u552f\u4e00\u7684\u968f\u673a\u6027\u6765\u6e90), \u505a\u7a7a \\(1\\) \u4efd\u671f\u6743, \u4e70\u5165 $ { \\partial V \\over \\partial S } $ \u4efd\u80a1\u7968 (Delta Neutral \u5bf9\u51b2), \u7ec4\u5408\u4ef7\u503c\u4e3a</li> </ul> \\[  \\Pi = -V + { \\partial V \\over \\partial S } S \\\\[5pt] \\Longrightarrow d\\Pi  = - \\left(         { \\partial V \\over \\partial t }                + {1\\over 2} \\sigma^2 S^2 { \\partial^2 V \\over \\partial S^2 }      \\right) dt \\]"},{"location":"trading/options.html#pi-r","title":"\u4ee4 \\(\\Pi\\) \u7684\u6536\u76ca\u7387\u7b49\u4e8e\u65e0\u98ce\u9669\u5229\u7387 \\(r\\)","text":"<ul> <li>\u65e0\u5957\u5229\u539f\u7406: BS\u6a21\u578b\u5e94\u5f53\u7ed9\u51fa\u671f\u6743\u6700\u516c\u5e73\u7684\u5b9a\u4ef7<ul> <li>\u65e0\u98ce\u9669\u5229\u7387 \\(r\\): \u7406\u60f3\u4e2d\u7684 \u94f6\u884c \u501f/\u5b58 \u5229\u7387</li> <li>\u82e5 $ d\\Pi &gt; r\\Pi dt $, \u53ef\u501f\u94b1\u5e76\u4e70\u5165\u7ec4\u5408 \\(\\Pi\\) \u5b9e\u73b0\u65e0\u98ce\u9669\u6536\u76ca</li> <li>\u82e5 $ d\\Pi &lt; r\\Pi dt $, \u53ef\u5356\u51fa\u7ec4\u5408 \\(\\Pi\\) \u5e76\u5b58\u94b1\u5b9e\u73b0\u65e0\u98ce\u9669\u6536\u76ca</li> <li>\u4ec5\u5f53 $ d\\Pi = r \\Pi dt $, \u6ca1\u6709\u65e0\u98ce\u9669\u5957\u5229\u673a\u4f1a</li> </ul> </li> </ul> \\[ d\\Pi = r \\Pi dt \\\\[5pt] \\Longrightarrow - \\left(         { \\partial V \\over \\partial t }                + {1\\over 2} \\sigma^2 S^2 { \\partial^2 V \\over \\partial S^2 }      \\right) dt = r (-V + { \\partial V \\over \\partial S } S) dt \\\\[5pt] \\text{Black-Scholes: } \\boxed{     { \\underbrace{ \\partial V \\over \\partial t }_{ -\\Theta } }            + {1\\over 2} \\sigma^2 S^2 {          \\underbrace{ \\partial^2 V \\over \\partial S^2 }_{ \\Gamma }      }      +  rS { \\underbrace{ \\partial V \\over \\partial S }_{ \\Delta } }       -rV  = 0 } \\\\[5pt] \\boxed{ \\nu := { \\partial V \\over \\partial \\sigma } \\quad \\rho := { \\partial V \\over \\partial r } } \\]"},{"location":"trading/options.html#_1","title":"\u65b9\u7a0b\u7684\u89e3 (\u4ec5\u601d\u8def \u65e0\u8fc7\u7a0b)","text":"<ul> <li> <p>\u6839\u636e\u671f\u6743\u7c7b\u578b\u8bbe\u5b9a\u8fb9\u754c\u6761\u4ef6, \u901a\u8fc7\u66ff\u6362\u53d8\u91cf\u8f6c\u5316\u4e3a\u70ed\u4f20\u5bfc\u65b9\u7a0b, \u4f7f\u7528\u79ef\u5206\u53d8\u6362(\u683c\u6797\u51fd\u6570\u6cd5)\u6c42\u89e3</p> </li> <li> <p>\u5bf9\u4e8e\u6b27\u5f0f\u770b\u6da8\u671f\u6743:</p> </li> </ul> \\[ \\boxed{ V(S, t) = S \\; N(d_1) - K e^{-r \\; T} \\; N(d_2) } \\\\[5pt] d_1 = { \\ln(S/K) + (r + \\sigma^2 / 2) T \\over \\sigma \\sqrt{ T } } \\\\[5pt] d_2 = d_1 - \\sigma \\sqrt{ T } \\\\[5pt] T := T_0 - t \\quad \\text{ time left }\\\\[5pt] N(\\cdot) \\quad \\text{ standard normal CDF } \\\\[5pt] \\boxed{ \\Delta = N(d_1) \\quad \\Gamma = { N'(d_1) \\over S \\sigma \\sqrt{ T } }  \\quad \\nu = S\\sqrt{T} N'(d_1) } \\\\[5pt] \\boxed{ \\Theta = - { S N'(d_1) \\sigma \\over 2\\sqrt{T} } - r K e^{-r T} N(d_2)  } \\]"},{"location":"trading/options.html#_2","title":"\u5b9e\u6218\u9898","text":"<ul> <li>\u5168\u90e8\u4ee3\u7801</li> </ul>"},{"location":"trading/options.html#1-bs","title":"1. BS \u6a21\u578b\u8ba1\u7b97\u9a8c\u8bc1","text":"<p>\u6295\u8d44\u4eba\u4e70\u8fdb1000\u5f20Option, \u80a1\u4ef7100, \u6267\u884c\u4ef7100, \u5230\u671f\u65e5180\u5929, \u6ce2\u52a8\u738716%, \u4e00\u5e74256\u5929, \u5229\u73877\uff05. \u5047\u5b9a\u6b64Option\u4e3aDelta Neutral\u72b6\u6001. \u7528python\u9a8c\u8bc1BS Model, \u6c42\u51fa\u5f53\u80a1\u4ef7\u6da8/\u8dcc 1% \u65f6, \u4e00\u65e5Theta, \u603bPL.</p> <ul> <li>\u7ed3\u679c</li> </ul> <pre><code>stock: 100 -&gt; 99.0\noption: 7.968044763858842 -&gt; 7.284780019739209\nstock_pl: 667.8328836634573\noption_pl: -683.264744119633\nTheta_pl + Delta_pl (-stock_pl) + Gamma_pl: -683.9151768360123\n    Theta_pl: -29.6140665687756\n    Gamma_pl: 13.531773396220588\ntotal_pl: -15.431860456175627\n\n\nstock: 100 -&gt; 101.0\noption: 7.968044763858842 -&gt; 8.619186079616412\nstock_pl: -667.8328836634573\noption_pl: 651.1413157575703\nTheta_pl + Delta_pl (-stock_pl) + Gamma_pl: 651.7505904909024\n    Theta_pl: -29.6140665687756\n    Gamma_pl: 13.531773396220588\ntotal_pl: -16.69156790588704\n</code></pre> <ul> <li>\u4ee3\u7801</li> </ul> <pre><code>from numpy import exp, log, sqrt\nfrom scipy.stats import norm\n\n\ndef bs_euro_call(S, K, r, sig, T):\n    cdf, pdf = norm.cdf, norm.pdf\n\n    A = sig * sqrt(T)\n    d1 = (log(S / K) + (r + sig**2 / 2) * T) / A\n    d2 = d1 - A\n    B = K * exp(-r * T) * cdf(d2)\n\n    V = S * cdf(d1) - B\n    Delta = cdf(d1)\n    Gamma = pdf(d1) / (S * A)\n    Theta = -(S * pdf(d1) * sig) / (2 * sqrt(T)) - r * B\n    Vega = S * sqrt(T) * pdf(d1)\n    return dict(V=V, Delta=Delta, Gamma=Gamma, Theta=Theta, Vega=Vega)\n\n\ndef delta_neutral_step(\n    S1,\n    T1,\n    S2,\n    T2,\n    K=100,\n    r=0.07,\n    sig=0.16,\n    n_option=1000,\n):\n    x1 = bs_euro_call(S1, K, r, sig, T1)\n    n_stock = -x1[\"Delta\"] * n_option\n    x2 = bs_euro_call(S2, K, r, sig, T2)\n    Gamma_pl = n_option * 0.5 * x1[\"Gamma\"] * (S2 - S1) ** 2\n    Theta_pl = n_option * x1[\"Theta\"] * (T1 - T2)\n    stock_pl = n_stock * (S2 - S1)\n    option_pl = n_option * (x2[\"V\"] - x1[\"V\"])\n    total_pl = stock_pl + option_pl\n    info = f\"\"\"\nstock: {S1} -&gt; {S2}\noption: {x1['V']} -&gt; {x2['V']}\nstock_pl: {stock_pl}\noption_pl: {option_pl}\nTheta_pl + Delta_pl (-stock_pl) + Gamma_pl: {Theta_pl - stock_pl + Gamma_pl}\n    Theta_pl: {Theta_pl}\n    Gamma_pl: {Gamma_pl}\ntotal_pl: {total_pl}\n\"\"\"\n    return dict(\n        S=S1,\n        V=x1[\"V\"],\n        n_stock=n_stock,\n        Gamma_pl=Gamma_pl,\n        Theta_pl=Theta_pl,\n        stock_pl=stock_pl,\n        option_pl=option_pl,\n        total_pl=total_pl,\n        info=info,\n    )\n\n\nif __name__ == \"__main__\":\n    S1, T1, T2 = 100, 180 / 256, (180 - 1) / 256\n    for change in [-0.01, 0.01]:\n        S2 = S1 * (1 + change)\n        res = delta_neutral_step(S1, T1, S2, T2)\n        print(res[\"info\"])\n</code></pre>"},{"location":"trading/options.html#2a","title":"2a. \u8ba1\u7b97\u80a1\u4ef7\u5e03\u6717\u8fd0\u52a8\u7684\u4e24\u79cd\u65b9\u6cd5","text":"<ul> <li>\u7ed3\u679c</li> </ul> <ul> <li>\u4ee3\u7801</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef stock_price1(S0, num, dt, r, sig):\n    S = np.zeros(num)\n    S[0] = S0\n    dW = np.random.normal(0, np.sqrt(dt), num)\n    for i in range(1, num):\n        dS = S[i - 1] * (r * dt + sig * dW[i])\n        S[i] = S[i - 1] + dS\n    return S\n\n\ndef stock_price2(S0, num, dt, r, sig):\n    t = np.arange(num) * dt\n    dW = np.random.normal(0, np.sqrt(dt), num)\n    dW[0] = 0\n    return S0 * np.exp((r - sig**2 / 2) * t + sig * np.cumsum(dW))\n\n\nif __name__ == \"__main__\":\n    S0, num, dt, r, sig = 1, 100, 0.01, 0.1, 0.1\n    np.random.seed(0)\n    S1 = stock_price1(S0, num, dt, r, sig)\n    np.random.seed(0)\n    S2 = stock_price2(S0, num, dt, r, sig)\n    plt.plot(S1, label=\"S1\")\n    plt.plot(S2, label=\"S2\")\n    plt.legend()\n    plt.savefig(\"question2a\")\n</code></pre>"},{"location":"trading/options.html#2b-delta","title":"2b. \u52a8\u6001 Delta \u5bf9\u51b2","text":"<p>\u80a1\u4ef7100, \u6267\u884c\u4ef7100, \u5230\u671f\u65e5180\u5929, \u6ce2\u52a8\u738716%, \u4e00\u5e74360\u5929, \u5229\u73877\uff05, \u4e70\u51651000\u5f20Option, \u907f\u9669\u9891\u7387\u4e3a1\u5929\u3002\u7528python\u6a21\u62dfBS Model\u4e0bOption\u7684Dynamic Delta Hedge\u8fc7\u7a0b.</p> <ul> <li> <p>\u7ed3\u679c1: \u5355\u6b21\u6a21\u62df</p> <ul> <li>excel \u6587\u4ef6</li> <li></li> <li><code>S</code>: \u80a1\u7968\u4ef7\u683c, \u7531 \u51e0\u4f55\u5e03\u6717\u8fd0\u52a8 \u751f\u6210    </li> <li><code>V</code>: \u671f\u6743\u4ef7\u683c (\u6b27\u5f0f\u770b\u6da8), \u7531\u80a1\u7968\u4ef7\u683c\u4ee5\u53ca BS \u516c\u5f0f\u7b97\u51fa </li> <li><code>n_stock</code>: (\u505a\u7a7a) \u5bf9\u51b2\u7684\u80a1\u7968\u6570\u91cf (\u7531\u4e8e\u4e70\u5165\u4e86\u6b27\u5f0f\u770b\u6da8\u671f\u6743)</li> <li><code>Gamma_pl</code>: \\(\\Gamma\\) \u6548\u5e94\u7684\u6bcf\u65e5\u6536\u76ca</li> <li><code>Theta_pl</code>: \\(\\Theta\\) \u6548\u5e94\u7684\u6bcf\u65e5\u6536\u76ca</li> <li><code>stock_pl</code>: \u6bcf\u65e5\u80a1\u7968\u4ef7\u683c\u53d8\u5316\u5e26\u6765\u7684\u6536\u76ca</li> <li><code>option_pl</code>: \u6bcf\u65e5\u671f\u6743\u4ef7\u683c\u53d8\u5316\u5e26\u6765\u7684\u6536\u76ca</li> <li><code>total_pl</code>: \u6bcf\u65e5\u603b\u6536\u76ca <code>total_pl = stock_pl + option_pl</code><ul> <li>\u53ef\u4ee5\u770b\u51fa, \u8fd9\u4e00\u9879\u7edd\u5bf9\u503c\u8fdc\u5c0f\u4e8e\u80a1\u7968/\u671f\u6743\u6536\u76ca\u7684\u7edd\u5bf9\u503c, \u4f53\u73b0\u4e86\u5bf9\u51b2</li> </ul> </li> <li><code>cum_total_pl</code>: \u7d2f\u8ba1\u603b\u6536\u76ca <code>cum_total_pl = cumsum(total_pl)</code></li> <li></li> </ul> </li> <li> <p>\u7ed3\u679c2: \u591a\u6b21\u6a21\u62df, \u4e0d\u8003\u8651\u671f\u6743\u6bcf\u65e5\u6536\u76ca, \u800c\u662f\u770b\u671f\u6743\u7684\u6700\u7ec8\u6267\u884c</p> </li> </ul> <pre><code>\u80a1\u7968\u5bf9\u51b2\u7d2f\u8ba1\u6536\u76ca: -13177.827686668968\n\u4e70\u5165\u671f\u6743\u7684\u6210\u672c: -6363.485217326122\n\u671f\u6743\u5230\u671f\u6267\u884c\u5e26\u6765\u7684\u6536\u76ca: 16783.48316247069\n\u6700\u7ec8\u603b\u6536\u76ca: -2757.8297415244015\n\n\u80a1\u7968\u5bf9\u51b2\u7d2f\u8ba1\u6536\u76ca: -10509.726762807535\n\u4e70\u5165\u671f\u6743\u7684\u6210\u672c: -6363.485217326122\n\u671f\u6743\u5230\u671f\u6267\u884c\u5e26\u6765\u7684\u6536\u76ca: 13973.358289900432\n\u6700\u7ec8\u603b\u6536\u76ca: -2899.853690233227\n\n\u80a1\u7968\u5bf9\u51b2\u7d2f\u8ba1\u6536\u76ca: 4262.71837293126\n\u4e70\u5165\u671f\u6743\u7684\u6210\u672c: -6363.485217326122\n\u671f\u6743\u5230\u671f\u6267\u884c\u5e26\u6765\u7684\u6536\u76ca: 1694.855159339852\n\u6700\u7ec8\u603b\u6536\u76ca: -405.91168505501037\n\n\u80a1\u7968\u5bf9\u51b2\u7d2f\u8ba1\u6536\u76ca: 5525.898204087093\n\u4e70\u5165\u671f\u6743\u7684\u6210\u672c: -6363.485217326122\n\u671f\u6743\u5230\u671f\u6267\u884c\u5e26\u6765\u7684\u6536\u76ca: 0\n\u6700\u7ec8\u603b\u6536\u76ca: -837.5870132390291\n\n\u80a1\u7968\u5bf9\u51b2\u7d2f\u8ba1\u6536\u76ca: -8050.688607611437\n\u4e70\u5165\u671f\u6743\u7684\u6210\u672c: -6363.485217326122\n\u671f\u6743\u5230\u671f\u6267\u884c\u5e26\u6765\u7684\u6536\u76ca: 11540.995496282576\n\u6700\u7ec8\u603b\u6536\u76ca: -2873.1783286549835\n</code></pre> <ul> <li>\u4ee3\u7801</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom question1 import delta_neutral_step\nfrom question2a import stock_price2\n\n\ndef dynamic_delta_hedging(\n    S0=100,\n    days=180,\n    DAYS=360,\n    K=100,\n    r=0.07,\n    sig=0.16,\n    n_option=1000,\n):\n    dt = 1 / DAYS\n    S = stock_price2(S0, days, dt, r, sig)\n    results = []\n    for i in range(days - 1):\n        S1, S2 = S[i], S[i + 1]\n        T1 = (days - i) / DAYS\n        T2 = (days - (i + 1)) / DAYS\n        x = delta_neutral_step(S1, T1, S2, T2, K, r, sig, n_option)\n        results.append(x)\n    df = pd.DataFrame(results).select_dtypes(include=[\"number\"])\n    df[\"cum_total_pl\"] = df[\"total_pl\"].cumsum()\n    df[\"cum_stock_pl\"] = df[\"stock_pl\"].cumsum()\n    R, C = df.shape\n    df.plot(subplots=True, figsize=(6, 3 * C))\n    plt.tight_layout()\n    plt.savefig(\"question2b\")\n    df.to_excel(\"dynamic_delta_hedging.xlsx\", index=False)\n\n    stock_pl2 = df[\"stock_pl\"].sum()\n    option_pl0 = -df[\"V\"].iat[0] * n_option\n    option_pl2 = max(S[-1] - K, 0) * n_option\n    final_pl = stock_pl2 + option_pl0 + option_pl2\n    print(f\"\u80a1\u7968\u5bf9\u51b2\u7d2f\u8ba1\u6536\u76ca: {stock_pl2}\")\n    print(f\"\u4e70\u5165\u671f\u6743\u7684\u6210\u672c: {option_pl0}\")\n    print(f\"\u671f\u6743\u5230\u671f\u6267\u884c\u5e26\u6765\u7684\u6536\u76ca: {option_pl2}\")\n    print(f\"\u6700\u7ec8\u603b\u6536\u76ca: {final_pl}\")\n\n\nif __name__ == \"__main__\":\n    for seed in range(5):\n        np.random.seed(seed)\n        dynamic_delta_hedging()\n        print()\n</code></pre>"},{"location":"trading/options.html#34-greeks-sigma-t","title":"3\u30014. Greeks\u4e0e \\(\\sigma, T\\) \u7684\u5173\u7cfb","text":"<p>\\(\\sigma , T\\) \u4e0eGreeks, \u5448\u4f55\u79cd\u5173\u7cfb? (\u7528\u516c\u5f0f\u8bf4\u660e)</p> <ul> <li>\u7ed3\u679c: \u7531\u4ee5\u4e0b\u516c\u5f0f\u53ef\u4ee5\u770b\u51fa\u5b83\u4eec\u7684\u5173\u7cfb, \u6bd4\u8d77\u5b9a\u6027\u63cf\u8ff0, \u6211\u8ba4\u4e3a\u76f4\u63a5\u753b\u56fe\u76f4\u89c2\u7406\u89e3\u66f4\u5408\u7406. \\(\\sigma\\) \u5de6\u56fe, \\(T\\) \u53f3\u56fe</li> </ul> \\[ \\boxed{ V(S, t) = S \\; N(d_1) - K e^{-r \\; T} \\; N(d_2) } \\\\[5pt] d_1 = { \\ln(S/K) + (r + \\sigma^2 / 2) T \\over \\sigma \\sqrt{ T } } \\\\[5pt] d_2 = d_1 - \\sigma \\sqrt{ T } \\\\[5pt] T := T_0 - t \\quad \\text{ time left }\\\\[5pt] N(\\cdot) \\quad \\text{ standard normal CDF } \\\\[5pt] \\boxed{ \\Delta = N(d_1) \\quad \\Gamma = { N'(d_1) \\over S \\sigma \\sqrt{ T } }  \\quad \\nu = S\\sqrt{T} N'(d_1) } \\\\[5pt] \\boxed{ \\Theta = - { S N'(d_1) \\sigma \\over 2\\sqrt{T} } - r K e^{-r T} N(d_2)  } \\] <ul> <li>\u4ee3\u7801</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom question1 import bs_euro_call\n\n\ndef greeks_vs_sigma():\n    K, r, T = 100, 0.07, 0.5\n    sig_arr = np.linspace(0.01, 0.3, 100)\n    keys = [\"Delta\", \"Gamma\", \"Vega\", \"Theta\"]\n    plt.figure(figsize=(6, 3 * len(keys)))\n    for i, k in enumerate(keys):\n        plt.subplot(len(keys), 1, i + 1)\n        plt.title(f\"{k} VS sigma\")\n        for ratio in [0.8, 1, 1.2]:\n            S = ratio * K\n            Y = [bs_euro_call(S, K, r, sig, T)[k] for sig in sig_arr]\n            plt.plot(sig_arr, Y, label=f\"S = {ratio} * K\")\n        plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"question3\")\n\n\ndef greeks_vs_T():\n    K, r, sig = 100, 0.07, 0.16\n    T_arr = np.linspace(0.01, 1, 100)\n    keys = [\"Delta\", \"Gamma\", \"Vega\", \"Theta\"]\n    plt.figure(figsize=(6, 3 * len(keys)))\n    for i, k in enumerate(keys):\n        plt.subplot(len(keys), 1, i + 1)\n        plt.title(f\"{k} VS T\")\n        for ratio in [0.8, 1, 1.2]:\n            S = ratio * K\n            Y = [bs_euro_call(S, K, r, sig, T)[k] for T in T_arr]\n            plt.plot(T_arr, Y, label=f\"S = {ratio} * K\")\n        plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"question4\")\n\n\nif __name__ == \"__main__\":\n    greeks_vs_sigma()\n    greeks_vs_T()\n</code></pre>"},{"location":"trading/options.html#5","title":"5. \u6ce2\u52a8\u7387\u5fae\u7b11","text":"<p>\u4ea7\u751fVolatility Smile\u7684\u539f\u56e0\u6709\u54ea\u4e9b?</p> <ul> <li>\u7ed3\u679c<ul> <li>\u9996\u5148\u6839\u636e\u771f\u5b9e\u6570\u636e\u753b\u56fe\u7406\u89e3 Volatility Smile, \u5e76\u8fdb\u884c\u4e86 SVI (Stochastic Volatility Inspired) \u6a21\u578b\u62df\u5408</li> <li>Black-Scholes \u5047\u8bbe\u7f3a\u9677  <ul> <li>BS\u6a21\u578b\u5047\u8bbe\u6ce2\u52a8\u7387 $ \\sigma $ \u4e3a\u5e38\u6570\uff0c\u4f46\u5b9e\u9645\u5e02\u573a\u4e2d\uff1a  <ul> <li>\u6781\u7aef\u4ef7\u683c\u8fd0\u52a8\uff08\u80a5\u5c3e\uff1a\u771f\u5b9e\u8d44\u4ea7\u56de\u62a5\u5206\u5e03\u5448\u73b0\u80a5\u5c3e\uff08\u5982\u5d29\u76d8/\u66b4\u6da8\uff09\uff0cBS\u5bf9\u6570\u6b63\u6001\u5047\u8bbe\u4f4e\u4f30\u5c3e\u90e8\u98ce\u9669 \u2192 \u6df1\u5ea6\u5b9e\u503c/\u865a\u503c\u671f\u6743\u9700\u66f4\u9ad8 $ \\sigma $ \u5b9a\u4ef7\u3002  </li> <li>\u8df3\u8dc3\u98ce\u9669\uff1a\u8d44\u4ea7\u4ef7\u683c\u5b58\u5728\u7a81\u53d1\u8df3\u8dc3\uff08\u5982\u8d22\u62a5\u53d1\u5e03\u3001\u9ed1\u5929\u9e45\u4e8b\u4ef6\uff09\uff0c\u8fde\u7eed\u6269\u6563\u5047\u8bbe\u5931\u6548 \u2192 \u865a\u503c\u671f\u6743\u9690\u542b\u6ce2\u52a8\u7387\u4e0a\u5347\u3002</li> </ul> </li> </ul> </li> <li>\u5e02\u573a\u4f9b\u9700\u52a8\u6001  <ul> <li>\u6050\u614c\u4e0d\u5bf9\u79f0\u6027\uff1a\u6295\u8d44\u8005\u5bf9\u4e0b\u8dcc\u6050\u614c\uff08\u865a\u503c\u770b\u8dcc\u671f\u6743\u9700\u6c42\u6fc0\u589e\uff09\uff1e \u4e0a\u6da8\u6295\u673a\uff08\u865a\u503c\u770b\u6da8\uff09\uff0c\u63a8\u9ad8OTM Put\u7684\u9690\u542b\u6ce2\u52a8\u7387\u3002  </li> <li>\u5bf9\u51b2\u538b\u529b\uff1a\u505a\u5e02\u5546\u5bf9\u51b2OTM\u671f\u6743\u9700\u52a8\u6001\u8c03\u6574\u5934\u5bf8\uff0c\u653e\u5927\u5e02\u573a\u6ce2\u52a8\uff0c\u53cd\u54fa $ \\sigma $\u3002</li> </ul> </li> <li>\u6d41\u52a8\u6027\u5206\u5c42  <ul> <li>\u6df1\u5ea6\u5b9e\u503c/\u865a\u503c\u671f\u6743\u6d41\u52a8\u6027\u5dee \u2192 \u4e70\u5356\u4ef7\u5dee\u6269\u5927 \u2192 \u901a\u8fc7BS\u516c\u5f0f\u53cd\u63a8 $ \\sigma $ \u65f6\u4eba\u4e3a\u62ac\u9ad8\u3002  </li> </ul> </li> <li>\u603b\u7ed3<ul> <li>\u6a21\u578b\u7f3a\u9677\uff08\u80a5\u5c3e/\u8df3\u8dc3\uff09\u2192 \u5e02\u573a\u884c\u4e3a\uff08\u6050\u614c/\u5bf9\u51b2\uff09\u2192 \u6d41\u52a8\u6027\u53cd\u9988 \u2192 \u9690\u542b\u6ce2\u52a8\u7387\u66f2\u7ebf\u626d\u66f2\u4e3aSmile\u3002</li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li>\u4ee3\u7801</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom scipy.optimize import minimize\n\n\ndef SVI(K, params):\n    a, b, rho, m, sigma = params\n    dK = K - m\n    return a + b * (rho * dK + np.sqrt(dK**2 + sigma**2))\n\n\ndef find_SVI_params(K, IV):\n    def obj(x):\n        return np.sum((SVI(K, x) - IV) ** 2)\n\n    x0 = [0.1, 0.1, -0.5, 0.0, 0.1]\n    bounds = [(0, None), (0, None), (-1, 1), (None, None), (0.01, None)]\n    return minimize(obj, x0, bounds=bounds).x\n\n\ndef get_option_data(ticker=\"TSLA\", n_expiry=2):\n    x = yf.Ticker(ticker)\n    for date in x.options[:n_expiry]:\n        df: pd.DataFrame = x.option_chain(date).calls\n        K = df[\"strike\"]\n        IV = df[\"impliedVolatility\"]\n        params = find_SVI_params(K, IV)\n        sig = SVI(K, params)\n        plt.plot(K, IV, label=f\"expire: {date}\")\n        plt.plot(K, sig, label=f\"expire: {date} (SVI)\")\n    plt.title(f\"Volatility smile of {ticker}\")\n    plt.xlabel(\"K\")\n    plt.ylabel(\"implied sigma\")\n    plt.legend()\n    plt.savefig(\"question5\")\n\n\nif __name__ == \"__main__\":\n    get_option_data()\n</code></pre>"},{"location":"trading/options.html#6-bs","title":"6. BS\u6a21\u578b\u7684\u57fa\u672c\u7406\u89e3","text":"<p>\u5047\u8bbeOption\u7684\u5b9a\u4ef7\u6ce2\u52a8\u738735%, \u65e0\u98ce\u9669\u5229\u73873%, \u5230\u671f\u65f6\u5df2\u5b9e\u73b0\u7684\u6ce2\u52a8\u7387\u4ea6\u4e3a35%, \u5219\u5728BS\u7684\u5047\u8bbe\u4e0b\u590d\u5236Option, \u6c42\u6b64\u6295\u8d44\u7ec4\u5408\u7684\u62a5\u916c\u7387\u3002(\u8bf7\u5199\u51fa\u5177\u4f53\u8bc1\u660e\u516c\u5f0f)</p> <ul> <li>\u7ed3\u679c<ul> <li>\u62a5\u916c\u7387 = \u65e0\u98ce\u9669\u5229\u7387 = 3%</li> <li>\u5728\u672c\u9875\u6700\u4e0a\u9762\u7684 BS \u7406\u8bba\u63a8\u5bfc\u4e2d, $ d\\Pi = r \\Pi dt $ \u662f BS \u5b9a\u4ef7\u7406\u8bba\u7684\u57fa\u672c\u5047\u8bbe/\u8981\u6c42, \u65e0\u9700\u8bc1\u660e.</li> </ul> </li> </ul>"},{"location":"trading/options.html#7-delta-neutral","title":"7. Delta Neutral \u7684\u98ce\u9669","text":"<p>\u6295\u8d44\u4eba\u5356\u51faOption \u5e76\u91c7\u53d6Delta Neutral\u7b56\u7565, \u53ef\u80fd\u6709\u54ea\u4e9b\u56e0\u7d20\u5bfc\u81f4\u5176\u98ce\u9669\u65e0\u6cd5\u5b8c\u5168\u5bf9\u51b2?</p> <ul> <li>\u7ed3\u679c<ul> <li>Gamma\u98ce\u9669: Delta\u968f\u6807\u7684\u8d44\u4ea7\u4ef7\u683c\u975e\u7ebf\u6027\u53d8\u5316\uff08\u4e8c\u9636\u6548\u5e94\uff09\uff0c\u5feb\u901f\u6ce2\u52a8\u65f6\u5bf9\u51b2\u6ede\u540e\u3002</li> <li>Vega\u98ce\u9669: \u9690\u542b\u6ce2\u52a8\u7387\u53d8\u5316\u5f71\u54cd\u671f\u6743\u4ef7\u503c\uff0cDelta\u5bf9\u51b2\u4e0d\u8986\u76d6\u6b64\u98ce\u9669\u3002</li> <li>\u65f6\u95f4\u8870\u51cf\uff08Theta\uff09: \u65f6\u95f4\u6d41\u901d\u5bf9\u671f\u6743\u5356\u65b9\u6709\u5229\uff0c\u4f46\u9700\u52a8\u6001\u8c03\u6574\u5bf9\u51b2\u9891\u7387\u4e0e\u6210\u672c\u5e73\u8861\u3002</li> <li>\u6d41\u52a8\u6027\u98ce\u9669: \u6807\u7684\u8d44\u4ea7\u6216\u671f\u6743\u6d41\u52a8\u6027\u4e0d\u8db3\u5bfc\u81f4\u65e0\u6cd5\u53ca\u65f6\u8c03\u6574\u5934\u5bf8\u3002</li> <li>\u8df3\u7a7a\u7f3a\u53e3: \u4ef7\u683c\u4e0d\u8fde\u7eed\u53d8\u52a8\uff08\u5982\u8d22\u62a5\u53d1\u5e03\uff09\u4f7f\u77ac\u65f6Delta\u5931\u6548\u3002</li> <li>\u5229\u7387\u4e0e\u80a1\u606f\u53d8\u5316: \u5f71\u54cd\u671f\u6743\u5b9a\u4ef7\u56e0\u5b50\uff0c\u5c24\u5176\u5bf9\u957f\u671f\u671f\u6743\u663e\u8457\u3002</li> <li>\u4ea4\u6613\u6210\u672c: \u9891\u7e41\u8c03\u4ed3\u635f\u8017\u5229\u6da6\uff0c\u964d\u4f4e\u5bf9\u51b2\u7cbe\u5ea6\u3002</li> <li>\u76f8\u5173\u6027\u98ce\u9669\uff08\u7ec4\u5408\u5bf9\u51b2\u65f6\uff09: \u5bf9\u51b2\u8d44\u4ea7\u4e0e\u6807\u7684\u5b9e\u9645\u76f8\u5173\u6027\u504f\u79bb\u9884\u671f\u3002</li> <li>\u6838\u5fc3\u77db\u76fe\uff1aDelta\u4ec5\u4e3a\u4e00\u9636\u8fd1\u4f3c\uff0c\u800c\u5e02\u573a\u662f\u591a\u56e0\u5b50\u52a8\u6001\u7cfb\u7edf\u3002</li> </ul> </li> </ul>"},{"location":"trading/options.html#8-vs","title":"8. \u770b\u6da8 vs \u770b\u8dcc","text":"<p>\u5927\u90e8\u5206\u5e02\u573a\u4e2d, Call\u7684\u9690\u542b\u6ce2\u52a8\u7387\u901a\u5e38\u8f83Put\u4f4e, \u539f\u56e0\u53ef\u80fd\u662f?</p> <ul> <li>\u7ed3\u679c<ul> <li>\u9700\u6c42\u5931\u8861\uff1a\u5e02\u573a\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u8d2d\u4e70Put\u5bf9\u51b2\u4e0b\u884c\u98ce\u9669\uff0c\u63a8\u9ad8Put\u6ea2\u4ef7\uff08\u9690\u542b\u6ce2\u52a8\u7387\u4e0a\u5347\uff09\u3002Call\u7684\u4e70\u65b9\uff08\u6295\u673a\u8005\uff09\u53ef\u80fd\u56e0\u6210\u672c\u654f\u611f\u800c\u538b\u4f4e\u9700\u6c42\u3002</li> <li>\u6760\u6746\u504f\u597d\uff1a\u6295\u673a\u8005\u66f4\u503e\u5411\u76f4\u63a5\u4e70\u80a1\u7968\u6216\u671f\u8d27\uff08\u800c\u975eCall\uff09\u83b7\u53d6\u6760\u6746\uff0c\u51cf\u5c11Call\u7684\u6ce2\u52a8\u7387\u6ea2\u4ef7\u3002</li> <li>\u5e02\u573a\u60c5\u7eea\uff1a\u6050\u614c\u60c5\u7eea\uff08\u5982\u5c3e\u90e8\u98ce\u9669\u62c5\u5fe7\uff09\u5bf9Put\u7684\u9700\u6c42\u5f71\u54cd\u66f4\u4e0d\u5bf9\u79f0\u3002</li> <li>\u5356\u538b\u5dee\u5f02\uff1a\u505a\u5e02\u5546\u5356\u51faCall\u65f6\u5bf9\u51b2\u6210\u672c\u66f4\u4f4e\uff08\u5982\u901a\u8fc7\u6301\u6709\u6b63\u80a1\uff09\uff0c\u5bfc\u81f4Call\u4f9b\u7ed9\u5f39\u6027\u66f4\u5927\u3002</li> <li>\u9690\u542b\u6ce2\u52a8\u7387\u5dee\u5f02\u672c\u8d28\u53cd\u6620\u4f9b\u9700\u4e0d\u5e73\u8861\uff0c\u800c\u975e\u6a21\u578b\u672c\u8eab\u7279\u6027</li> </ul> </li> </ul>"},{"location":"trading/options.html#9-vs","title":"9. \u671f\u8d27 vs \u671f\u6743","text":"<p>\u660e\u5929\u4e3a\u6807\u7684\u671f\u8d27\u7ed3\u7b97\u65e5, \u6295\u8d44\u4eba\u9884\u671f\u8be5\u6807\u7684\u5c06\u6da82%, \u4e3a\u4f7f\u62a5\u916c\u7387\u6700\u5927, \u4f1a\u9009\u62e9\u671f\u8d27\u8fd8\u662f\u671f\u6743?</p> <ul> <li>\u7ed3\u679c<ul> <li>\u5bf9\u53ef\u80fd\u7684\u60c5\u51b5\u8fdb\u884c\u753b\u56fe</li> <li>\u4e3a\u4e86\u6700\u5927\u5316\u6536\u76ca, \u5e94\u9009\u62e9options, \u56e0\u4e3aoptions\u6760\u6746\u66f4\u5927 (\u4f46\u6f5c\u5728\u98ce\u9669\u4e5f\u66f4\u5927)</li> </ul> </li> </ul> <p></p> <ul> <li>\u4ee3\u7801</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef futures_vs_options(\n    S0=100,  # \u5f53\u524d\u6807\u7684\u8d44\u4ea7\u4ef7\u683c\n    K=102,  # \u865a\u503c\u770b\u6da8\u671f\u6743\u884c\u6743\u4ef7\uff08\u9884\u671f\u6da82%\uff09\n    margin_ratio=0.12,  # \u671f\u8d27\u4fdd\u8bc1\u91d1\u6bd4\u4f8b\uff0812%\uff09\n    V=0.5,  # \u865a\u503c\u671f\u6743\u6743\u5229\u91d1\uff080.5% of S0\uff09\n):\n    # \u6807\u7684\u7ed3\u7b97\u4ef7\u8303\u56f4\uff1a-5%\u5230+5%\n    S = S0 * (1 + np.linspace(-0.05, 0.05, 100))\n\n    futures_pl = (S - S0) / (margin_ratio * S0)\n    option_pl = (np.maximum(S - K, 0) - V) / V\n\n    plt.plot(S, futures_pl, label=\"futures pl\")\n    plt.plot(S, option_pl, label=\"options pl\")\n    plt.axvline(K, linestyle=\"--\", label=\"K=102\")\n    plt.xlabel(\"S\")\n    plt.ylabel(\"PnL\")\n    plt.title(\"futures vs options\")\n    plt.legend()\n    plt.savefig(\"question9\")\n\n\nif __name__ == \"__main__\":\n    futures_vs_options()\n</code></pre>"}]}